{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satisfactory-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.py\n",
    "# Modified from\n",
    "# https://github.com/pytorch/vision/blob/release/0.8.0/torchvision/models/resnet.py\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        # Rename relu to relu1\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "        # Remember to use two independent ReLU for layer fusion.\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        # Use FloatFunctional for addition for quantization compatibility\n",
    "        # out += identity\n",
    "        out = self.skip_add.add(identity, out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.relu2  = nn.ReLU(inplace=True)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # out += identity\n",
    "        out = self.skip_add.add(identity, out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intellectual-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedResNet50(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedResNet50, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.model_fp32 = model_fp32\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generic-tumor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasuy\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\quantization\\observer.py:119: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizedResNet50(\n",
      "  (quant): Quantize(scale=tensor([0.0375]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "  (model_fp32): ResNet(\n",
      "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.018436769023537636, zero_point=0, padding=(3, 3))\n",
      "    (bn1): Identity()\n",
      "    (relu): Identity()\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.010135799646377563, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.011952046304941177, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.020515695214271545, zero_point=59)\n",
      "        (bn3): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.032741695642471313, zero_point=76)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.035102978348731995, zero_point=72\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.00922879483550787, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.015914831310510635, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.016127794981002808, zero_point=64)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.022906972095370293, zero_point=45\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.009117220528423786, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.019783396273851395, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.02355561964213848, zero_point=69)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.02851303294301033, zero_point=57\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.011866655200719833, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.009378612972795963, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.02354905940592289, zero_point=61)\n",
      "        (bn3): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.021553104743361473, zero_point=57)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.028694937005639076, zero_point=59\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.006868576630949974, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.010652955621480942, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.030405322089791298, zero_point=70)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.030345192179083824, zero_point=63\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.009477490559220314, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.00895635038614273, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.018680300563573837, zero_point=62)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.027333330363035202, zero_point=45\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.0100365299731493, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.008711608126759529, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.016849905252456665, zero_point=67)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.02557417005300522, zero_point=46\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.01329004392027855, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.010932079516351223, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.021548030897974968, zero_point=59)\n",
      "        (bn3): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), scale=0.016776524484157562, zero_point=59)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.024471191689372063, zero_point=61\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.010810386389493942, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.010484698228538036, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.027499347925186157, zero_point=64)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.02869166061282158, zero_point=52\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.008823418989777565, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.00716364523395896, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.013658547773957253, zero_point=65)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.023126672953367233, zero_point=39\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.00954610388725996, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.007442679721862078, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.018794028088450432, zero_point=69)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.025851933285593987, zero_point=45\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.010821557603776455, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.008071394637227058, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.01842919923365116, zero_point=73)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.026974745094776154, zero_point=47\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.014104763977229595, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.015504616312682629, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.026358334347605705, zero_point=79)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.032445620745420456, zero_point=61\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.012715116143226624, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.01450834795832634, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.056098680943250656, zero_point=52)\n",
      "        (bn3): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), scale=0.05205056816339493, zero_point=56)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.07223725318908691, zero_point=49\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.012333572842180729, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.007715554907917976, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.10453417897224426, zero_point=44)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.08634493499994278, zero_point=29\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.011701620183885098, zero_point=0)\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.008856935426592827, zero_point=0, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (relu2): Identity()\n",
      "        (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.11785325407981873, zero_point=39)\n",
      "        (bn3): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.15148378908634186, zero_point=30\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu3): QuantizedReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): QuantizedLinear(in_features=2048, out_features=1000, scale=0.2643173635005951, zero_point=31, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "\n",
    "#model_fp32 = models.resnet18(pretrained=True).to('cpu')\n",
    "model_fp32 = resnet50(pretrained=True).to('cpu')\n",
    "#print(model_fp32)\n",
    "#print(list(model_fp32.children()))\n",
    "\n",
    "model_fp32.eval()\n",
    "\n",
    "\n",
    "fused_model = torch.quantization.fuse_modules(model_fp32, [\n",
    "    ['conv1', 'bn1', 'relu'],\n",
    "    ['layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu1'],\n",
    "    ['layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.relu2'],\n",
    "    ['layer1.0.conv3', 'layer1.0.bn3'],\n",
    "    ['layer1.0.downsample.0', 'layer1.0.downsample.1'],\n",
    "    ['layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu1'],\n",
    "    ['layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.relu2'],\n",
    "    ['layer1.1.conv3', 'layer1.1.bn3'],\n",
    "    ['layer1.2.conv1', 'layer1.2.bn1', 'layer1.2.relu1'],\n",
    "    ['layer1.2.conv2', 'layer1.2.bn2', 'layer1.2.relu2'],\n",
    "    ['layer1.2.conv3', 'layer1.2.bn3'],\n",
    "    ['layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu1'],\n",
    "    ['layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.relu2'],\n",
    "    ['layer2.0.conv3', 'layer2.0.bn3'],\n",
    "    ['layer2.0.downsample.0', 'layer2.0.downsample.1'],\n",
    "    ['layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu1'],\n",
    "    ['layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.relu2'],\n",
    "    ['layer2.1.conv3', 'layer2.1.bn3'],\n",
    "    ['layer2.2.conv1', 'layer2.2.bn1', 'layer2.2.relu1'],\n",
    "    ['layer2.2.conv2', 'layer2.2.bn2', 'layer2.2.relu2'],\n",
    "    ['layer2.2.conv3', 'layer2.2.bn3'],\n",
    "    ['layer2.3.conv1', 'layer2.3.bn1', 'layer2.3.relu1'],\n",
    "    ['layer2.3.conv2', 'layer2.3.bn2', 'layer2.3.relu2'],\n",
    "    ['layer2.3.conv3', 'layer2.3.bn3'],\n",
    "    ['layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu1'],\n",
    "    ['layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.relu2'],\n",
    "    ['layer3.0.conv3', 'layer3.0.bn3'],\n",
    "    ['layer3.0.downsample.0', 'layer3.0.downsample.1'],\n",
    "    ['layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu1'],\n",
    "    ['layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.relu2'],\n",
    "    ['layer3.1.conv3', 'layer3.1.bn3'],\n",
    "    ['layer3.2.conv1', 'layer3.2.bn1', 'layer3.2.relu1'],\n",
    "    ['layer3.2.conv2', 'layer3.2.bn2', 'layer3.2.relu2'],\n",
    "    ['layer3.2.conv3', 'layer3.2.bn3'],\n",
    "    ['layer3.3.conv1', 'layer3.3.bn1', 'layer3.3.relu1'],\n",
    "    ['layer3.3.conv2', 'layer3.3.bn2', 'layer3.3.relu2'],\n",
    "    ['layer3.3.conv3', 'layer3.3.bn3'],\n",
    "    ['layer3.4.conv1', 'layer3.4.bn1', 'layer3.4.relu1'],\n",
    "    ['layer3.4.conv2', 'layer3.4.bn2', 'layer3.4.relu2'],\n",
    "    ['layer3.4.conv3', 'layer3.4.bn3'],\n",
    "    ['layer3.5.conv1', 'layer3.5.bn1', 'layer3.5.relu1'],\n",
    "    ['layer3.5.conv2', 'layer3.5.bn2', 'layer3.5.relu2'],\n",
    "    ['layer3.5.conv3', 'layer3.5.bn3'],\n",
    "    ['layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu1'],\n",
    "    ['layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.relu2'],\n",
    "    ['layer4.0.conv3', 'layer4.0.bn3'],\n",
    "    ['layer4.0.downsample.0', 'layer4.0.downsample.1'],\n",
    "    ['layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu1'],\n",
    "    ['layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.relu2'],\n",
    "    ['layer4.1.conv3', 'layer4.1.bn3'],\n",
    "    ['layer4.2.conv1', 'layer4.2.bn1', 'layer4.2.relu1'],\n",
    "    ['layer4.2.conv2', 'layer4.2.bn2', 'layer4.2.relu2'],\n",
    "    ['layer4.2.conv3', 'layer4.2.bn3'],\n",
    "], inplace=True)\n",
    "\n",
    "\n",
    "#print(list(fused_model.children()))\n",
    "\n",
    "quantized_model = QuantizedResNet50(model_fp32=fused_model)\n",
    "#print(list(quantized_model.children()))\n",
    "\n",
    "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "quantized_model.qconfig = quantization_config\n",
    "#print(quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(quantized_model, inplace=True)\n",
    "\n",
    "# calibrate_model(model=quantized_model, loader=train_loader, device=cpu_device)\n",
    "quantized_model.to('cpu')\n",
    "quantized_model.eval()\n",
    "\n",
    "for index in range(1000):\n",
    "    file = './Imagenet/' + str(index) + '.jpg'\n",
    "    img = Image.open(file)\n",
    "    img_tensor = preprocess(img)\n",
    "    img_tensor.unsqueeze_(0)\n",
    "    _ = quantized_model(Variable(img_tensor).to('cpu'))\n",
    "\n",
    "\n",
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
    "#print(list(quantized_model.children()))\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "marine-nowhere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[19.0308, 11.1013,  8.7225,  8.1938,  7.1366]]),\n",
      "indices=tensor([[  0, 389, 391, 394, 395]]))\n",
      "Top1: 1  Top5: 1  All: 1\n",
      "TOP1 ACCURACY: 1 / 1000\n",
      "TOP5 ACCURACY: 1 / 1000\n",
      "AVERAGE TIME: 5.398740000009638e-05\n",
      "INVALID DATA: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class_index = json.load(open('imagenet_class_index.json', 'r'))\n",
    "\n",
    "correct_top1 = 0\n",
    "correct_top5 = 0\n",
    "average_time = 0\n",
    "invalid = 0\n",
    "invalid_wnid = []\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "\n",
    "quantized_model.eval()\n",
    "for index in range(1000):\n",
    "    #file = './Imagenet/' + str(index) + '.jpg'\n",
    "    file = './1000/' + str(index) + '.txt'\n",
    "    try:\n",
    "        #img = Image.open(file)\n",
    "        #img_tensor = preprocess(img)\n",
    "        #img_tensor.unsqueeze_(0)\n",
    "        \n",
    "        img = np.loadtxt(file, dtype='float32')\n",
    "        img = img.reshape(224, 224, 3)\n",
    "        img_tensor = torch.from_numpy(img.astype(np.float32))\n",
    "        img_tensor = img_tensor.unsqueeze(0).permute(0,3,1,2)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        outputs = quantized_model(Variable(img_tensor).to('cpu'))\n",
    "        end = time.perf_counter()\n",
    "        average_time += end - start\n",
    "        #print(outputs)\n",
    "\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        softmax_out = softmax(outputs)\n",
    "        #print(outputs.topk(5))\n",
    "\n",
    "        for i in range(5):\n",
    "            imagenet_key = str(softmax_out.topk(5)[1][0][i].item())\n",
    "            if imagenet_key == str(index):\n",
    "                if i == 0:\n",
    "                    correct_top1 += 1\n",
    "                correct_top5 += 1\n",
    "        print('Top1: ' + str(correct_top1) + '  Top5: ' + str(correct_top5) + '  All: ' + str(index+1))\n",
    "    except:\n",
    "        invalid += 1\n",
    "        #print(class_index[str(index)][0])\n",
    "        invalid_wnid.append(class_index[str(index)][0])\n",
    "\n",
    "print('TOP1 ACCURACY: ' + str(correct_top1) + ' / ' + str(1000-invalid))\n",
    "print('TOP5 ACCURACY: ' + str(correct_top5) + ' / ' + str(1000-invalid))\n",
    "print('AVERAGE TIME: ' + str(average_time/1000))\n",
    "print('INVALID DATA: ' + str(invalid))\n",
    "print(invalid_wnid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-recruitment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "present-broadcast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant.scale\n",
      "quant.zero_point\n",
      "model_fp32.conv1.weight\n",
      "model_fp32.conv1.bias\n",
      "model_fp32.conv1.scale\n",
      "model_fp32.conv1.zero_point\n",
      "model_fp32.layer1.0.conv1.weight\n",
      "model_fp32.layer1.0.conv1.bias\n",
      "model_fp32.layer1.0.conv1.scale\n",
      "model_fp32.layer1.0.conv1.zero_point\n",
      "model_fp32.layer1.0.conv2.weight\n",
      "model_fp32.layer1.0.conv2.bias\n",
      "model_fp32.layer1.0.conv2.scale\n",
      "model_fp32.layer1.0.conv2.zero_point\n",
      "model_fp32.layer1.0.conv3.weight\n",
      "model_fp32.layer1.0.conv3.bias\n",
      "model_fp32.layer1.0.conv3.scale\n",
      "model_fp32.layer1.0.conv3.zero_point\n",
      "model_fp32.layer1.0.downsample.0.weight\n",
      "model_fp32.layer1.0.downsample.0.bias\n",
      "model_fp32.layer1.0.downsample.0.scale\n",
      "model_fp32.layer1.0.downsample.0.zero_point\n",
      "model_fp32.layer1.0.skip_add.scale\n",
      "model_fp32.layer1.0.skip_add.zero_point\n",
      "model_fp32.layer1.1.conv1.weight\n",
      "model_fp32.layer1.1.conv1.bias\n",
      "model_fp32.layer1.1.conv1.scale\n",
      "model_fp32.layer1.1.conv1.zero_point\n",
      "model_fp32.layer1.1.conv2.weight\n",
      "model_fp32.layer1.1.conv2.bias\n",
      "model_fp32.layer1.1.conv2.scale\n",
      "model_fp32.layer1.1.conv2.zero_point\n",
      "model_fp32.layer1.1.conv3.weight\n",
      "model_fp32.layer1.1.conv3.bias\n",
      "model_fp32.layer1.1.conv3.scale\n",
      "model_fp32.layer1.1.conv3.zero_point\n",
      "model_fp32.layer1.1.skip_add.scale\n",
      "model_fp32.layer1.1.skip_add.zero_point\n",
      "model_fp32.layer1.2.conv1.weight\n",
      "model_fp32.layer1.2.conv1.bias\n",
      "model_fp32.layer1.2.conv1.scale\n",
      "model_fp32.layer1.2.conv1.zero_point\n",
      "model_fp32.layer1.2.conv2.weight\n",
      "model_fp32.layer1.2.conv2.bias\n",
      "model_fp32.layer1.2.conv2.scale\n",
      "model_fp32.layer1.2.conv2.zero_point\n",
      "model_fp32.layer1.2.conv3.weight\n",
      "model_fp32.layer1.2.conv3.bias\n",
      "model_fp32.layer1.2.conv3.scale\n",
      "model_fp32.layer1.2.conv3.zero_point\n",
      "model_fp32.layer1.2.skip_add.scale\n",
      "model_fp32.layer1.2.skip_add.zero_point\n",
      "model_fp32.layer2.0.conv1.weight\n",
      "model_fp32.layer2.0.conv1.bias\n",
      "model_fp32.layer2.0.conv1.scale\n",
      "model_fp32.layer2.0.conv1.zero_point\n",
      "model_fp32.layer2.0.conv2.weight\n",
      "model_fp32.layer2.0.conv2.bias\n",
      "model_fp32.layer2.0.conv2.scale\n",
      "model_fp32.layer2.0.conv2.zero_point\n",
      "model_fp32.layer2.0.conv3.weight\n",
      "model_fp32.layer2.0.conv3.bias\n",
      "model_fp32.layer2.0.conv3.scale\n",
      "model_fp32.layer2.0.conv3.zero_point\n",
      "model_fp32.layer2.0.downsample.0.weight\n",
      "model_fp32.layer2.0.downsample.0.bias\n",
      "model_fp32.layer2.0.downsample.0.scale\n",
      "model_fp32.layer2.0.downsample.0.zero_point\n",
      "model_fp32.layer2.0.skip_add.scale\n",
      "model_fp32.layer2.0.skip_add.zero_point\n",
      "model_fp32.layer2.1.conv1.weight\n",
      "model_fp32.layer2.1.conv1.bias\n",
      "model_fp32.layer2.1.conv1.scale\n",
      "model_fp32.layer2.1.conv1.zero_point\n",
      "model_fp32.layer2.1.conv2.weight\n",
      "model_fp32.layer2.1.conv2.bias\n",
      "model_fp32.layer2.1.conv2.scale\n",
      "model_fp32.layer2.1.conv2.zero_point\n",
      "model_fp32.layer2.1.conv3.weight\n",
      "model_fp32.layer2.1.conv3.bias\n",
      "model_fp32.layer2.1.conv3.scale\n",
      "model_fp32.layer2.1.conv3.zero_point\n",
      "model_fp32.layer2.1.skip_add.scale\n",
      "model_fp32.layer2.1.skip_add.zero_point\n",
      "model_fp32.layer2.2.conv1.weight\n",
      "model_fp32.layer2.2.conv1.bias\n",
      "model_fp32.layer2.2.conv1.scale\n",
      "model_fp32.layer2.2.conv1.zero_point\n",
      "model_fp32.layer2.2.conv2.weight\n",
      "model_fp32.layer2.2.conv2.bias\n",
      "model_fp32.layer2.2.conv2.scale\n",
      "model_fp32.layer2.2.conv2.zero_point\n",
      "model_fp32.layer2.2.conv3.weight\n",
      "model_fp32.layer2.2.conv3.bias\n",
      "model_fp32.layer2.2.conv3.scale\n",
      "model_fp32.layer2.2.conv3.zero_point\n",
      "model_fp32.layer2.2.skip_add.scale\n",
      "model_fp32.layer2.2.skip_add.zero_point\n",
      "model_fp32.layer2.3.conv1.weight\n",
      "model_fp32.layer2.3.conv1.bias\n",
      "model_fp32.layer2.3.conv1.scale\n",
      "model_fp32.layer2.3.conv1.zero_point\n",
      "model_fp32.layer2.3.conv2.weight\n",
      "model_fp32.layer2.3.conv2.bias\n",
      "model_fp32.layer2.3.conv2.scale\n",
      "model_fp32.layer2.3.conv2.zero_point\n",
      "model_fp32.layer2.3.conv3.weight\n",
      "model_fp32.layer2.3.conv3.bias\n",
      "model_fp32.layer2.3.conv3.scale\n",
      "model_fp32.layer2.3.conv3.zero_point\n",
      "model_fp32.layer2.3.skip_add.scale\n",
      "model_fp32.layer2.3.skip_add.zero_point\n",
      "model_fp32.layer3.0.conv1.weight\n",
      "model_fp32.layer3.0.conv1.bias\n",
      "model_fp32.layer3.0.conv1.scale\n",
      "model_fp32.layer3.0.conv1.zero_point\n",
      "model_fp32.layer3.0.conv2.weight\n",
      "model_fp32.layer3.0.conv2.bias\n",
      "model_fp32.layer3.0.conv2.scale\n",
      "model_fp32.layer3.0.conv2.zero_point\n",
      "model_fp32.layer3.0.conv3.weight\n",
      "model_fp32.layer3.0.conv3.bias\n",
      "model_fp32.layer3.0.conv3.scale\n",
      "model_fp32.layer3.0.conv3.zero_point\n",
      "model_fp32.layer3.0.downsample.0.weight\n",
      "model_fp32.layer3.0.downsample.0.bias\n",
      "model_fp32.layer3.0.downsample.0.scale\n",
      "model_fp32.layer3.0.downsample.0.zero_point\n",
      "model_fp32.layer3.0.skip_add.scale\n",
      "model_fp32.layer3.0.skip_add.zero_point\n",
      "model_fp32.layer3.1.conv1.weight\n",
      "model_fp32.layer3.1.conv1.bias\n",
      "model_fp32.layer3.1.conv1.scale\n",
      "model_fp32.layer3.1.conv1.zero_point\n",
      "model_fp32.layer3.1.conv2.weight\n",
      "model_fp32.layer3.1.conv2.bias\n",
      "model_fp32.layer3.1.conv2.scale\n",
      "model_fp32.layer3.1.conv2.zero_point\n",
      "model_fp32.layer3.1.conv3.weight\n",
      "model_fp32.layer3.1.conv3.bias\n",
      "model_fp32.layer3.1.conv3.scale\n",
      "model_fp32.layer3.1.conv3.zero_point\n",
      "model_fp32.layer3.1.skip_add.scale\n",
      "model_fp32.layer3.1.skip_add.zero_point\n",
      "model_fp32.layer3.2.conv1.weight\n",
      "model_fp32.layer3.2.conv1.bias\n",
      "model_fp32.layer3.2.conv1.scale\n",
      "model_fp32.layer3.2.conv1.zero_point\n",
      "model_fp32.layer3.2.conv2.weight\n",
      "model_fp32.layer3.2.conv2.bias\n",
      "model_fp32.layer3.2.conv2.scale\n",
      "model_fp32.layer3.2.conv2.zero_point\n",
      "model_fp32.layer3.2.conv3.weight\n",
      "model_fp32.layer3.2.conv3.bias\n",
      "model_fp32.layer3.2.conv3.scale\n",
      "model_fp32.layer3.2.conv3.zero_point\n",
      "model_fp32.layer3.2.skip_add.scale\n",
      "model_fp32.layer3.2.skip_add.zero_point\n",
      "model_fp32.layer3.3.conv1.weight\n",
      "model_fp32.layer3.3.conv1.bias\n",
      "model_fp32.layer3.3.conv1.scale\n",
      "model_fp32.layer3.3.conv1.zero_point\n",
      "model_fp32.layer3.3.conv2.weight\n",
      "model_fp32.layer3.3.conv2.bias\n",
      "model_fp32.layer3.3.conv2.scale\n",
      "model_fp32.layer3.3.conv2.zero_point\n",
      "model_fp32.layer3.3.conv3.weight\n",
      "model_fp32.layer3.3.conv3.bias\n",
      "model_fp32.layer3.3.conv3.scale\n",
      "model_fp32.layer3.3.conv3.zero_point\n",
      "model_fp32.layer3.3.skip_add.scale\n",
      "model_fp32.layer3.3.skip_add.zero_point\n",
      "model_fp32.layer3.4.conv1.weight\n",
      "model_fp32.layer3.4.conv1.bias\n",
      "model_fp32.layer3.4.conv1.scale\n",
      "model_fp32.layer3.4.conv1.zero_point\n",
      "model_fp32.layer3.4.conv2.weight\n",
      "model_fp32.layer3.4.conv2.bias\n",
      "model_fp32.layer3.4.conv2.scale\n",
      "model_fp32.layer3.4.conv2.zero_point\n",
      "model_fp32.layer3.4.conv3.weight\n",
      "model_fp32.layer3.4.conv3.bias\n",
      "model_fp32.layer3.4.conv3.scale\n",
      "model_fp32.layer3.4.conv3.zero_point\n",
      "model_fp32.layer3.4.skip_add.scale\n",
      "model_fp32.layer3.4.skip_add.zero_point\n",
      "model_fp32.layer3.5.conv1.weight\n",
      "model_fp32.layer3.5.conv1.bias\n",
      "model_fp32.layer3.5.conv1.scale\n",
      "model_fp32.layer3.5.conv1.zero_point\n",
      "model_fp32.layer3.5.conv2.weight\n",
      "model_fp32.layer3.5.conv2.bias\n",
      "model_fp32.layer3.5.conv2.scale\n",
      "model_fp32.layer3.5.conv2.zero_point\n",
      "model_fp32.layer3.5.conv3.weight\n",
      "model_fp32.layer3.5.conv3.bias\n",
      "model_fp32.layer3.5.conv3.scale\n",
      "model_fp32.layer3.5.conv3.zero_point\n",
      "model_fp32.layer3.5.skip_add.scale\n",
      "model_fp32.layer3.5.skip_add.zero_point\n",
      "model_fp32.layer4.0.conv1.weight\n",
      "model_fp32.layer4.0.conv1.bias\n",
      "model_fp32.layer4.0.conv1.scale\n",
      "model_fp32.layer4.0.conv1.zero_point\n",
      "model_fp32.layer4.0.conv2.weight\n",
      "model_fp32.layer4.0.conv2.bias\n",
      "model_fp32.layer4.0.conv2.scale\n",
      "model_fp32.layer4.0.conv2.zero_point\n",
      "model_fp32.layer4.0.conv3.weight\n",
      "model_fp32.layer4.0.conv3.bias\n",
      "model_fp32.layer4.0.conv3.scale\n",
      "model_fp32.layer4.0.conv3.zero_point\n",
      "model_fp32.layer4.0.downsample.0.weight\n",
      "model_fp32.layer4.0.downsample.0.bias\n",
      "model_fp32.layer4.0.downsample.0.scale\n",
      "model_fp32.layer4.0.downsample.0.zero_point\n",
      "model_fp32.layer4.0.skip_add.scale\n",
      "model_fp32.layer4.0.skip_add.zero_point\n",
      "model_fp32.layer4.1.conv1.weight\n",
      "model_fp32.layer4.1.conv1.bias\n",
      "model_fp32.layer4.1.conv1.scale\n",
      "model_fp32.layer4.1.conv1.zero_point\n",
      "model_fp32.layer4.1.conv2.weight\n",
      "model_fp32.layer4.1.conv2.bias\n",
      "model_fp32.layer4.1.conv2.scale\n",
      "model_fp32.layer4.1.conv2.zero_point\n",
      "model_fp32.layer4.1.conv3.weight\n",
      "model_fp32.layer4.1.conv3.bias\n",
      "model_fp32.layer4.1.conv3.scale\n",
      "model_fp32.layer4.1.conv3.zero_point\n",
      "model_fp32.layer4.1.skip_add.scale\n",
      "model_fp32.layer4.1.skip_add.zero_point\n",
      "model_fp32.layer4.2.conv1.weight\n",
      "model_fp32.layer4.2.conv1.bias\n",
      "model_fp32.layer4.2.conv1.scale\n",
      "model_fp32.layer4.2.conv1.zero_point\n",
      "model_fp32.layer4.2.conv2.weight\n",
      "model_fp32.layer4.2.conv2.bias\n",
      "model_fp32.layer4.2.conv2.scale\n",
      "model_fp32.layer4.2.conv2.zero_point\n",
      "model_fp32.layer4.2.conv3.weight\n",
      "model_fp32.layer4.2.conv3.bias\n",
      "model_fp32.layer4.2.conv3.scale\n",
      "model_fp32.layer4.2.conv3.zero_point\n",
      "model_fp32.layer4.2.skip_add.scale\n",
      "model_fp32.layer4.2.skip_add.zero_point\n",
      "model_fp32.fc.scale\n",
      "model_fp32.fc.zero_point\n",
      "model_fp32.fc._packed_params.dtype\n",
      "model_fp32.fc._packed_params._packed_params\n"
     ]
    }
   ],
   "source": [
    "# SHOW ALL KEY\n",
    "import numpy as np\n",
    "for key in quantized_model.state_dict():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-arcade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "defined-marijuana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.037455279380083084\n"
     ]
    }
   ],
   "source": [
    "#VIEW PARAMS\n",
    "import numpy as np\n",
    "key = 'quant.scale'\n",
    "print(quantized_model.state_dict()[key].item())\n",
    "#print(np.array(quantized_model.state_dict()[key].int_repr()))\n",
    "#print(quantized_model.state_dict()[key].q_per_channel_scales())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "major-probability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.conv3.weight\n",
      "layer1.0.downsample.0.weight\n",
      "layer1.1.conv1.weight\n",
      "layer1.1.conv2.weight\n",
      "layer1.1.conv3.weight\n",
      "layer1.2.conv1.weight\n",
      "layer1.2.conv2.weight\n",
      "layer1.2.conv3.weight\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.conv3.weight\n",
      "layer2.0.downsample.0.weight\n",
      "layer2.1.conv1.weight\n",
      "layer2.1.conv2.weight\n",
      "layer2.1.conv3.weight\n",
      "layer2.2.conv1.weight\n",
      "layer2.2.conv2.weight\n",
      "layer2.2.conv3.weight\n",
      "layer2.3.conv1.weight\n",
      "layer2.3.conv2.weight\n",
      "layer2.3.conv3.weight\n",
      "layer3.0.conv1.weight\n",
      "layer3.0.conv2.weight\n",
      "layer3.0.conv3.weight\n",
      "layer3.0.downsample.0.weight\n",
      "layer3.1.conv1.weight\n",
      "layer3.1.conv2.weight\n",
      "layer3.1.conv3.weight\n",
      "layer3.2.conv1.weight\n",
      "layer3.2.conv2.weight\n",
      "layer3.2.conv3.weight\n",
      "layer3.3.conv1.weight\n",
      "layer3.3.conv2.weight\n",
      "layer3.3.conv3.weight\n",
      "layer3.4.conv1.weight\n",
      "layer3.4.conv2.weight\n",
      "layer3.4.conv3.weight\n",
      "layer3.5.conv1.weight\n",
      "layer3.5.conv2.weight\n",
      "layer3.5.conv3.weight\n",
      "layer4.0.conv1.weight\n",
      "layer4.0.conv2.weight\n",
      "layer4.0.conv3.weight\n",
      "layer4.0.downsample.0.weight\n",
      "layer4.1.conv1.weight\n",
      "layer4.1.conv2.weight\n",
      "layer4.1.conv3.weight\n",
      "layer4.2.conv1.weight\n",
      "layer4.2.conv2.weight\n",
      "layer4.2.conv3.weight\n"
     ]
    }
   ],
   "source": [
    "# SAVE WEIGHT\n",
    "import numpy as np\n",
    "for key in quantized_model.state_dict():\n",
    "    if key[-6:] == 'weight':\n",
    "        wfilename = key[11:]\n",
    "        np.savetxt('./resnet50int8-param/'+wfilename+'.txt', np.ravel(np.array(quantized_model.state_dict()[key].int_repr(), dtype='int8')))\n",
    "        np.savetxt('./resnet50int8-param/'+wfilename[:-6]+'scale.txt', np.ravel(np.array(quantized_model.state_dict()[key].q_per_channel_scales(), dtype='float32')))\n",
    "        print(wfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "revised-voluntary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias\n",
      "layer1.0.conv1.bias\n",
      "layer1.0.conv2.bias\n",
      "layer1.0.conv3.bias\n",
      "layer1.0.downsample.0.bias\n",
      "layer1.1.conv1.bias\n",
      "layer1.1.conv2.bias\n",
      "layer1.1.conv3.bias\n",
      "layer1.2.conv1.bias\n",
      "layer1.2.conv2.bias\n",
      "layer1.2.conv3.bias\n",
      "layer2.0.conv1.bias\n",
      "layer2.0.conv2.bias\n",
      "layer2.0.conv3.bias\n",
      "layer2.0.downsample.0.bias\n",
      "layer2.1.conv1.bias\n",
      "layer2.1.conv2.bias\n",
      "layer2.1.conv3.bias\n",
      "layer2.2.conv1.bias\n",
      "layer2.2.conv2.bias\n",
      "layer2.2.conv3.bias\n",
      "layer2.3.conv1.bias\n",
      "layer2.3.conv2.bias\n",
      "layer2.3.conv3.bias\n",
      "layer3.0.conv1.bias\n",
      "layer3.0.conv2.bias\n",
      "layer3.0.conv3.bias\n",
      "layer3.0.downsample.0.bias\n",
      "layer3.1.conv1.bias\n",
      "layer3.1.conv2.bias\n",
      "layer3.1.conv3.bias\n",
      "layer3.2.conv1.bias\n",
      "layer3.2.conv2.bias\n",
      "layer3.2.conv3.bias\n",
      "layer3.3.conv1.bias\n",
      "layer3.3.conv2.bias\n",
      "layer3.3.conv3.bias\n",
      "layer3.4.conv1.bias\n",
      "layer3.4.conv2.bias\n",
      "layer3.4.conv3.bias\n",
      "layer3.5.conv1.bias\n",
      "layer3.5.conv2.bias\n",
      "layer3.5.conv3.bias\n",
      "layer4.0.conv1.bias\n",
      "layer4.0.conv2.bias\n",
      "layer4.0.conv3.bias\n",
      "layer4.0.downsample.0.bias\n",
      "layer4.1.conv1.bias\n",
      "layer4.1.conv2.bias\n",
      "layer4.1.conv3.bias\n",
      "layer4.2.conv1.bias\n",
      "layer4.2.conv2.bias\n",
      "layer4.2.conv3.bias\n"
     ]
    }
   ],
   "source": [
    "# SAVE BIAS\n",
    "import numpy as np\n",
    "\n",
    "for key in quantized_model.state_dict():\n",
    "    if key[-4:] == 'bias':\n",
    "        wfilename = key[11:]\n",
    "        np.savetxt('./resnet50int8-param/'+wfilename+'.txt', np.ravel(quantized_model.state_dict()[key].to('cpu').detach().numpy().copy()))\n",
    "        print(wfilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "graduate-causing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2643)\n",
      "tensor(31)\n",
      "torch.qint8\n",
      "tensor([[-0.0091,  0.0159, -0.0364,  ..., -0.0114,  0.0000, -0.0273],\n",
      "        [ 0.0110, -0.0276,  0.0000,  ...,  0.0276, -0.0221, -0.0055],\n",
      "        [-0.0066,  0.0658, -0.0395,  ...,  0.0132, -0.0241, -0.0022],\n",
      "        ...,\n",
      "        [-0.0098, -0.0020, -0.0354,  ..., -0.0177,  0.1063,  0.0276],\n",
      "        [-0.0177, -0.0059, -0.0295,  ..., -0.0206, -0.0059, -0.0147],\n",
      "        [-0.0226,  0.0226,  0.0806,  ...,  0.0000, -0.0452,  0.0194]],\n",
      "       size=(1000, 2048), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0023, 0.0028, 0.0022, 0.0021, 0.0022, 0.0023, 0.0022, 0.0019, 0.0022,\n",
      "        0.0020, 0.0020, 0.0024, 0.0025, 0.0025, 0.0023, 0.0018, 0.0020, 0.0023,\n",
      "        0.0017, 0.0027, 0.0019, 0.0021, 0.0018, 0.0020, 0.0019, 0.0018, 0.0018,\n",
      "        0.0020, 0.0017, 0.0021, 0.0018, 0.0018, 0.0018, 0.0019, 0.0020, 0.0020,\n",
      "        0.0020, 0.0019, 0.0017, 0.0017, 0.0014, 0.0018, 0.0017, 0.0017, 0.0015,\n",
      "        0.0019, 0.0018, 0.0021, 0.0015, 0.0018, 0.0019, 0.0020, 0.0018, 0.0020,\n",
      "        0.0019, 0.0019, 0.0024, 0.0023, 0.0023, 0.0016, 0.0018, 0.0024, 0.0022,\n",
      "        0.0019, 0.0017, 0.0020, 0.0019, 0.0019, 0.0019, 0.0024, 0.0024, 0.0018,\n",
      "        0.0024, 0.0027, 0.0025, 0.0021, 0.0022, 0.0023, 0.0015, 0.0018, 0.0021,\n",
      "        0.0016, 0.0015, 0.0025, 0.0023, 0.0018, 0.0016, 0.0025, 0.0022, 0.0017,\n",
      "        0.0019, 0.0019, 0.0019, 0.0018, 0.0021, 0.0020, 0.0019, 0.0017, 0.0022,\n",
      "        0.0014, 0.0017, 0.0024, 0.0022, 0.0018, 0.0019, 0.0031, 0.0016, 0.0024,\n",
      "        0.0023, 0.0021, 0.0029, 0.0026, 0.0019, 0.0020, 0.0016, 0.0037, 0.0028,\n",
      "        0.0025, 0.0027, 0.0023, 0.0023, 0.0026, 0.0027, 0.0021, 0.0023, 0.0019,\n",
      "        0.0015, 0.0016, 0.0021, 0.0021, 0.0022, 0.0019, 0.0020, 0.0017, 0.0019,\n",
      "        0.0017, 0.0019, 0.0018, 0.0024, 0.0014, 0.0017, 0.0019, 0.0018, 0.0016,\n",
      "        0.0018, 0.0018, 0.0019, 0.0016, 0.0018, 0.0022, 0.0017, 0.0019, 0.0016,\n",
      "        0.0018, 0.0014, 0.0016, 0.0019, 0.0019, 0.0020, 0.0014, 0.0018, 0.0017,\n",
      "        0.0016, 0.0016, 0.0030, 0.0015, 0.0014, 0.0014, 0.0016, 0.0018, 0.0019,\n",
      "        0.0018, 0.0016, 0.0016, 0.0016, 0.0014, 0.0016, 0.0017, 0.0028, 0.0018,\n",
      "        0.0019, 0.0017, 0.0018, 0.0023, 0.0023, 0.0018, 0.0015, 0.0017, 0.0017,\n",
      "        0.0019, 0.0016, 0.0019, 0.0017, 0.0016, 0.0015, 0.0015, 0.0017, 0.0014,\n",
      "        0.0018, 0.0018, 0.0013, 0.0016, 0.0014, 0.0018, 0.0015, 0.0016, 0.0023,\n",
      "        0.0018, 0.0020, 0.0021, 0.0029, 0.0017, 0.0017, 0.0018, 0.0017, 0.0016,\n",
      "        0.0018, 0.0020, 0.0017, 0.0020, 0.0017, 0.0019, 0.0018, 0.0018, 0.0017,\n",
      "        0.0018, 0.0016, 0.0018, 0.0020, 0.0021, 0.0017, 0.0017, 0.0014, 0.0019,\n",
      "        0.0018, 0.0015, 0.0017, 0.0021, 0.0018, 0.0017, 0.0017, 0.0018, 0.0017,\n",
      "        0.0015, 0.0016, 0.0018, 0.0019, 0.0015, 0.0014, 0.0019, 0.0014, 0.0020,\n",
      "        0.0015, 0.0019, 0.0018, 0.0017, 0.0017, 0.0013, 0.0016, 0.0017, 0.0016,\n",
      "        0.0018, 0.0015, 0.0014, 0.0015, 0.0018, 0.0021, 0.0016, 0.0017, 0.0017,\n",
      "        0.0018, 0.0021, 0.0018, 0.0021, 0.0016, 0.0016, 0.0016, 0.0015, 0.0014,\n",
      "        0.0017, 0.0014, 0.0023, 0.0022, 0.0016, 0.0025, 0.0025, 0.0021, 0.0026,\n",
      "        0.0018, 0.0019, 0.0017, 0.0019, 0.0022, 0.0018, 0.0020, 0.0018, 0.0021,\n",
      "        0.0019, 0.0020, 0.0018, 0.0021, 0.0020, 0.0025, 0.0018, 0.0025, 0.0026,\n",
      "        0.0027, 0.0021, 0.0019, 0.0021, 0.0018, 0.0014, 0.0016, 0.0021, 0.0021,\n",
      "        0.0023, 0.0018, 0.0024, 0.0018, 0.0019, 0.0020, 0.0019, 0.0018, 0.0016,\n",
      "        0.0017, 0.0020, 0.0022, 0.0019, 0.0017, 0.0022, 0.0018, 0.0016, 0.0022,\n",
      "        0.0019, 0.0020, 0.0019, 0.0017, 0.0018, 0.0019, 0.0021, 0.0019, 0.0024,\n",
      "        0.0025, 0.0025, 0.0018, 0.0023, 0.0020, 0.0022, 0.0018, 0.0019, 0.0021,\n",
      "        0.0025, 0.0019, 0.0018, 0.0023, 0.0017, 0.0016, 0.0019, 0.0017, 0.0019,\n",
      "        0.0015, 0.0021, 0.0021, 0.0024, 0.0022, 0.0019, 0.0017, 0.0017, 0.0016,\n",
      "        0.0019, 0.0018, 0.0024, 0.0019, 0.0019, 0.0019, 0.0021, 0.0016, 0.0020,\n",
      "        0.0017, 0.0019, 0.0020, 0.0016, 0.0018, 0.0019, 0.0020, 0.0022, 0.0021,\n",
      "        0.0020, 0.0020, 0.0022, 0.0019, 0.0018, 0.0019, 0.0028, 0.0017, 0.0019,\n",
      "        0.0024, 0.0022, 0.0027, 0.0029, 0.0020, 0.0019, 0.0026, 0.0024, 0.0023,\n",
      "        0.0023, 0.0020, 0.0017, 0.0019, 0.0026, 0.0024, 0.0016, 0.0015, 0.0022,\n",
      "        0.0023, 0.0023, 0.0033, 0.0024, 0.0018, 0.0028, 0.0028, 0.0027, 0.0025,\n",
      "        0.0020, 0.0023, 0.0019, 0.0017, 0.0023, 0.0018, 0.0021, 0.0029, 0.0021,\n",
      "        0.0021, 0.0028, 0.0029, 0.0037, 0.0015, 0.0020, 0.0019, 0.0028, 0.0025,\n",
      "        0.0022, 0.0018, 0.0021, 0.0025, 0.0020, 0.0023, 0.0019, 0.0023, 0.0020,\n",
      "        0.0028, 0.0024, 0.0021, 0.0024, 0.0026, 0.0025, 0.0028, 0.0020, 0.0021,\n",
      "        0.0020, 0.0017, 0.0025, 0.0027, 0.0019, 0.0020, 0.0028, 0.0025, 0.0027,\n",
      "        0.0029, 0.0021, 0.0025, 0.0021, 0.0018, 0.0020, 0.0024, 0.0021, 0.0024,\n",
      "        0.0017, 0.0032, 0.0015, 0.0020, 0.0020, 0.0014, 0.0020, 0.0020, 0.0016,\n",
      "        0.0025, 0.0023, 0.0027, 0.0039, 0.0024, 0.0025, 0.0020, 0.0023, 0.0022,\n",
      "        0.0022, 0.0019, 0.0017, 0.0022, 0.0020, 0.0023, 0.0022, 0.0023, 0.0025,\n",
      "        0.0021, 0.0023, 0.0022, 0.0019, 0.0030, 0.0023, 0.0022, 0.0019, 0.0017,\n",
      "        0.0020, 0.0020, 0.0032, 0.0021, 0.0016, 0.0020, 0.0025, 0.0021, 0.0022,\n",
      "        0.0019, 0.0027, 0.0022, 0.0021, 0.0019, 0.0018, 0.0017, 0.0021, 0.0020,\n",
      "        0.0016, 0.0023, 0.0036, 0.0025, 0.0018, 0.0019, 0.0024, 0.0024, 0.0028,\n",
      "        0.0026, 0.0018, 0.0023, 0.0026, 0.0026, 0.0022, 0.0027, 0.0029, 0.0025,\n",
      "        0.0018, 0.0019, 0.0022, 0.0022, 0.0023, 0.0026, 0.0023, 0.0030, 0.0022,\n",
      "        0.0019, 0.0023, 0.0021, 0.0021, 0.0031, 0.0019, 0.0026, 0.0029, 0.0020,\n",
      "        0.0032, 0.0021, 0.0023, 0.0025, 0.0018, 0.0018, 0.0025, 0.0018, 0.0024,\n",
      "        0.0024, 0.0020, 0.0025, 0.0020, 0.0032, 0.0022, 0.0017, 0.0022, 0.0016,\n",
      "        0.0018, 0.0018, 0.0018, 0.0045, 0.0018, 0.0017, 0.0025, 0.0024, 0.0023,\n",
      "        0.0028, 0.0021, 0.0023, 0.0015, 0.0026, 0.0033, 0.0014, 0.0024, 0.0032,\n",
      "        0.0020, 0.0022, 0.0018, 0.0028, 0.0021, 0.0058, 0.0018, 0.0024, 0.0042,\n",
      "        0.0021, 0.0017, 0.0019, 0.0021, 0.0023, 0.0027, 0.0017, 0.0019, 0.0022,\n",
      "        0.0018, 0.0020, 0.0020, 0.0023, 0.0032, 0.0020, 0.0023, 0.0022, 0.0022,\n",
      "        0.0022, 0.0021, 0.0019, 0.0017, 0.0027, 0.0017, 0.0015, 0.0018, 0.0017,\n",
      "        0.0020, 0.0024, 0.0023, 0.0028, 0.0023, 0.0019, 0.0023, 0.0029, 0.0021,\n",
      "        0.0023, 0.0017, 0.0019, 0.0024, 0.0029, 0.0020, 0.0017, 0.0018, 0.0020,\n",
      "        0.0021, 0.0025, 0.0023, 0.0019, 0.0016, 0.0019, 0.0018, 0.0023, 0.0021,\n",
      "        0.0016, 0.0024, 0.0022, 0.0026, 0.0022, 0.0019, 0.0024, 0.0017, 0.0019,\n",
      "        0.0018, 0.0019, 0.0021, 0.0022, 0.0028, 0.0016, 0.0018, 0.0023, 0.0020,\n",
      "        0.0034, 0.0021, 0.0018, 0.0020, 0.0018, 0.0020, 0.0021, 0.0016, 0.0017,\n",
      "        0.0016, 0.0020, 0.0020, 0.0023, 0.0022, 0.0023, 0.0021, 0.0036, 0.0020,\n",
      "        0.0031, 0.0022, 0.0018, 0.0021, 0.0022, 0.0020, 0.0021, 0.0016, 0.0023,\n",
      "        0.0022, 0.0027, 0.0018, 0.0018, 0.0023, 0.0034, 0.0017, 0.0020, 0.0023,\n",
      "        0.0021, 0.0025, 0.0028, 0.0021, 0.0026, 0.0021, 0.0020, 0.0018, 0.0033,\n",
      "        0.0019, 0.0023, 0.0017, 0.0017, 0.0020, 0.0019, 0.0026, 0.0024, 0.0020,\n",
      "        0.0023, 0.0021, 0.0023, 0.0026, 0.0019, 0.0019, 0.0020, 0.0019, 0.0023,\n",
      "        0.0019, 0.0019, 0.0021, 0.0020, 0.0020, 0.0021, 0.0020, 0.0016, 0.0018,\n",
      "        0.0019, 0.0016, 0.0019, 0.0017, 0.0036, 0.0017, 0.0021, 0.0020, 0.0018,\n",
      "        0.0024, 0.0023, 0.0018, 0.0023, 0.0029, 0.0027, 0.0016, 0.0022, 0.0024,\n",
      "        0.0019, 0.0021, 0.0025, 0.0020, 0.0020, 0.0046, 0.0024, 0.0019, 0.0030,\n",
      "        0.0020, 0.0024, 0.0026, 0.0027, 0.0024, 0.0024, 0.0030, 0.0039, 0.0021,\n",
      "        0.0019, 0.0027, 0.0032, 0.0022, 0.0019, 0.0024, 0.0021, 0.0018, 0.0021,\n",
      "        0.0021, 0.0026, 0.0025, 0.0015, 0.0020, 0.0023, 0.0019, 0.0028, 0.0018,\n",
      "        0.0034, 0.0018, 0.0021, 0.0018, 0.0022, 0.0029, 0.0023, 0.0024, 0.0023,\n",
      "        0.0019, 0.0021, 0.0018, 0.0027, 0.0019, 0.0027, 0.0023, 0.0019, 0.0019,\n",
      "        0.0028, 0.0018, 0.0016, 0.0029, 0.0022, 0.0021, 0.0033, 0.0017, 0.0038,\n",
      "        0.0038, 0.0021, 0.0019, 0.0021, 0.0019, 0.0027, 0.0032, 0.0018, 0.0019,\n",
      "        0.0020, 0.0020, 0.0015, 0.0020, 0.0017, 0.0039, 0.0032, 0.0033, 0.0033,\n",
      "        0.0021, 0.0024, 0.0019, 0.0047, 0.0027, 0.0025, 0.0023, 0.0033, 0.0032,\n",
      "        0.0020, 0.0025, 0.0022, 0.0022, 0.0020, 0.0017, 0.0019, 0.0024, 0.0019,\n",
      "        0.0019, 0.0022, 0.0030, 0.0034, 0.0025, 0.0036, 0.0023, 0.0020, 0.0025,\n",
      "        0.0016, 0.0019, 0.0019, 0.0038, 0.0018, 0.0018, 0.0020, 0.0023, 0.0023,\n",
      "        0.0026, 0.0023, 0.0016, 0.0025, 0.0024, 0.0028, 0.0027, 0.0017, 0.0015,\n",
      "        0.0022, 0.0020, 0.0018, 0.0033, 0.0024, 0.0029, 0.0023, 0.0029, 0.0023,\n",
      "        0.0022, 0.0025, 0.0025, 0.0031, 0.0025, 0.0022, 0.0025, 0.0031, 0.0022,\n",
      "        0.0020, 0.0018, 0.0022, 0.0025, 0.0025, 0.0019, 0.0016, 0.0027, 0.0018,\n",
      "        0.0018, 0.0020, 0.0021, 0.0022, 0.0019, 0.0026, 0.0021, 0.0019, 0.0018,\n",
      "        0.0027, 0.0028, 0.0025, 0.0025, 0.0022, 0.0026, 0.0025, 0.0022, 0.0016,\n",
      "        0.0019, 0.0021, 0.0021, 0.0026, 0.0022, 0.0034, 0.0030, 0.0024, 0.0027,\n",
      "        0.0035, 0.0024, 0.0023, 0.0028, 0.0024, 0.0016, 0.0033, 0.0023, 0.0023,\n",
      "        0.0018, 0.0021, 0.0036, 0.0024, 0.0017, 0.0016, 0.0022, 0.0024, 0.0024,\n",
      "        0.0025, 0.0023, 0.0028, 0.0017, 0.0025, 0.0021, 0.0020, 0.0019, 0.0019,\n",
      "        0.0024, 0.0022, 0.0020, 0.0028, 0.0020, 0.0023, 0.0025, 0.0021, 0.0023,\n",
      "        0.0019, 0.0024, 0.0024, 0.0022, 0.0023, 0.0027, 0.0026, 0.0020, 0.0029,\n",
      "        0.0032], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -4,   7, -16,  ...,  -5,   0, -12],\n",
      "        [  4, -10,   0,  ...,  10,  -8,  -2],\n",
      "        [ -3,  30, -18,  ...,   6, -11,  -1],\n",
      "        ...,\n",
      "        [ -5,  -1, -18,  ...,  -9,  54,  14],\n",
      "        [ -6,  -2, -10,  ...,  -7,  -2,  -5],\n",
      "        [ -7,   7,  25,  ...,   0, -14,   6]], dtype=torch.int8)\n",
      "tensor([0.0023, 0.0028, 0.0022, 0.0021, 0.0022, 0.0023, 0.0022, 0.0019, 0.0022,\n",
      "        0.0020, 0.0020, 0.0024, 0.0025, 0.0025, 0.0023, 0.0018, 0.0020, 0.0023,\n",
      "        0.0017, 0.0027, 0.0019, 0.0021, 0.0018, 0.0020, 0.0019, 0.0018, 0.0018,\n",
      "        0.0020, 0.0017, 0.0021, 0.0018, 0.0018, 0.0018, 0.0019, 0.0020, 0.0020,\n",
      "        0.0020, 0.0019, 0.0017, 0.0017, 0.0014, 0.0018, 0.0017, 0.0017, 0.0015,\n",
      "        0.0019, 0.0018, 0.0021, 0.0015, 0.0018, 0.0019, 0.0020, 0.0018, 0.0020,\n",
      "        0.0019, 0.0019, 0.0024, 0.0023, 0.0023, 0.0016, 0.0018, 0.0024, 0.0022,\n",
      "        0.0019, 0.0017, 0.0020, 0.0019, 0.0019, 0.0019, 0.0024, 0.0024, 0.0018,\n",
      "        0.0024, 0.0027, 0.0025, 0.0021, 0.0022, 0.0023, 0.0015, 0.0018, 0.0021,\n",
      "        0.0016, 0.0015, 0.0025, 0.0023, 0.0018, 0.0016, 0.0025, 0.0022, 0.0017,\n",
      "        0.0019, 0.0019, 0.0019, 0.0018, 0.0021, 0.0020, 0.0019, 0.0017, 0.0022,\n",
      "        0.0014, 0.0017, 0.0024, 0.0022, 0.0018, 0.0019, 0.0031, 0.0016, 0.0024,\n",
      "        0.0023, 0.0021, 0.0029, 0.0026, 0.0019, 0.0020, 0.0016, 0.0037, 0.0028,\n",
      "        0.0025, 0.0027, 0.0023, 0.0023, 0.0026, 0.0027, 0.0021, 0.0023, 0.0019,\n",
      "        0.0015, 0.0016, 0.0021, 0.0021, 0.0022, 0.0019, 0.0020, 0.0017, 0.0019,\n",
      "        0.0017, 0.0019, 0.0018, 0.0024, 0.0014, 0.0017, 0.0019, 0.0018, 0.0016,\n",
      "        0.0018, 0.0018, 0.0019, 0.0016, 0.0018, 0.0022, 0.0017, 0.0019, 0.0016,\n",
      "        0.0018, 0.0014, 0.0016, 0.0019, 0.0019, 0.0020, 0.0014, 0.0018, 0.0017,\n",
      "        0.0016, 0.0016, 0.0030, 0.0015, 0.0014, 0.0014, 0.0016, 0.0018, 0.0019,\n",
      "        0.0018, 0.0016, 0.0016, 0.0016, 0.0014, 0.0016, 0.0017, 0.0028, 0.0018,\n",
      "        0.0019, 0.0017, 0.0018, 0.0023, 0.0023, 0.0018, 0.0015, 0.0017, 0.0017,\n",
      "        0.0019, 0.0016, 0.0019, 0.0017, 0.0016, 0.0015, 0.0015, 0.0017, 0.0014,\n",
      "        0.0018, 0.0018, 0.0013, 0.0016, 0.0014, 0.0018, 0.0015, 0.0016, 0.0023,\n",
      "        0.0018, 0.0020, 0.0021, 0.0029, 0.0017, 0.0017, 0.0018, 0.0017, 0.0016,\n",
      "        0.0018, 0.0020, 0.0017, 0.0020, 0.0017, 0.0019, 0.0018, 0.0018, 0.0017,\n",
      "        0.0018, 0.0016, 0.0018, 0.0020, 0.0021, 0.0017, 0.0017, 0.0014, 0.0019,\n",
      "        0.0018, 0.0015, 0.0017, 0.0021, 0.0018, 0.0017, 0.0017, 0.0018, 0.0017,\n",
      "        0.0015, 0.0016, 0.0018, 0.0019, 0.0015, 0.0014, 0.0019, 0.0014, 0.0020,\n",
      "        0.0015, 0.0019, 0.0018, 0.0017, 0.0017, 0.0013, 0.0016, 0.0017, 0.0016,\n",
      "        0.0018, 0.0015, 0.0014, 0.0015, 0.0018, 0.0021, 0.0016, 0.0017, 0.0017,\n",
      "        0.0018, 0.0021, 0.0018, 0.0021, 0.0016, 0.0016, 0.0016, 0.0015, 0.0014,\n",
      "        0.0017, 0.0014, 0.0023, 0.0022, 0.0016, 0.0025, 0.0025, 0.0021, 0.0026,\n",
      "        0.0018, 0.0019, 0.0017, 0.0019, 0.0022, 0.0018, 0.0020, 0.0018, 0.0021,\n",
      "        0.0019, 0.0020, 0.0018, 0.0021, 0.0020, 0.0025, 0.0018, 0.0025, 0.0026,\n",
      "        0.0027, 0.0021, 0.0019, 0.0021, 0.0018, 0.0014, 0.0016, 0.0021, 0.0021,\n",
      "        0.0023, 0.0018, 0.0024, 0.0018, 0.0019, 0.0020, 0.0019, 0.0018, 0.0016,\n",
      "        0.0017, 0.0020, 0.0022, 0.0019, 0.0017, 0.0022, 0.0018, 0.0016, 0.0022,\n",
      "        0.0019, 0.0020, 0.0019, 0.0017, 0.0018, 0.0019, 0.0021, 0.0019, 0.0024,\n",
      "        0.0025, 0.0025, 0.0018, 0.0023, 0.0020, 0.0022, 0.0018, 0.0019, 0.0021,\n",
      "        0.0025, 0.0019, 0.0018, 0.0023, 0.0017, 0.0016, 0.0019, 0.0017, 0.0019,\n",
      "        0.0015, 0.0021, 0.0021, 0.0024, 0.0022, 0.0019, 0.0017, 0.0017, 0.0016,\n",
      "        0.0019, 0.0018, 0.0024, 0.0019, 0.0019, 0.0019, 0.0021, 0.0016, 0.0020,\n",
      "        0.0017, 0.0019, 0.0020, 0.0016, 0.0018, 0.0019, 0.0020, 0.0022, 0.0021,\n",
      "        0.0020, 0.0020, 0.0022, 0.0019, 0.0018, 0.0019, 0.0028, 0.0017, 0.0019,\n",
      "        0.0024, 0.0022, 0.0027, 0.0029, 0.0020, 0.0019, 0.0026, 0.0024, 0.0023,\n",
      "        0.0023, 0.0020, 0.0017, 0.0019, 0.0026, 0.0024, 0.0016, 0.0015, 0.0022,\n",
      "        0.0023, 0.0023, 0.0033, 0.0024, 0.0018, 0.0028, 0.0028, 0.0027, 0.0025,\n",
      "        0.0020, 0.0023, 0.0019, 0.0017, 0.0023, 0.0018, 0.0021, 0.0029, 0.0021,\n",
      "        0.0021, 0.0028, 0.0029, 0.0037, 0.0015, 0.0020, 0.0019, 0.0028, 0.0025,\n",
      "        0.0022, 0.0018, 0.0021, 0.0025, 0.0020, 0.0023, 0.0019, 0.0023, 0.0020,\n",
      "        0.0028, 0.0024, 0.0021, 0.0024, 0.0026, 0.0025, 0.0028, 0.0020, 0.0021,\n",
      "        0.0020, 0.0017, 0.0025, 0.0027, 0.0019, 0.0020, 0.0028, 0.0025, 0.0027,\n",
      "        0.0029, 0.0021, 0.0025, 0.0021, 0.0018, 0.0020, 0.0024, 0.0021, 0.0024,\n",
      "        0.0017, 0.0032, 0.0015, 0.0020, 0.0020, 0.0014, 0.0020, 0.0020, 0.0016,\n",
      "        0.0025, 0.0023, 0.0027, 0.0039, 0.0024, 0.0025, 0.0020, 0.0023, 0.0022,\n",
      "        0.0022, 0.0019, 0.0017, 0.0022, 0.0020, 0.0023, 0.0022, 0.0023, 0.0025,\n",
      "        0.0021, 0.0023, 0.0022, 0.0019, 0.0030, 0.0023, 0.0022, 0.0019, 0.0017,\n",
      "        0.0020, 0.0020, 0.0032, 0.0021, 0.0016, 0.0020, 0.0025, 0.0021, 0.0022,\n",
      "        0.0019, 0.0027, 0.0022, 0.0021, 0.0019, 0.0018, 0.0017, 0.0021, 0.0020,\n",
      "        0.0016, 0.0023, 0.0036, 0.0025, 0.0018, 0.0019, 0.0024, 0.0024, 0.0028,\n",
      "        0.0026, 0.0018, 0.0023, 0.0026, 0.0026, 0.0022, 0.0027, 0.0029, 0.0025,\n",
      "        0.0018, 0.0019, 0.0022, 0.0022, 0.0023, 0.0026, 0.0023, 0.0030, 0.0022,\n",
      "        0.0019, 0.0023, 0.0021, 0.0021, 0.0031, 0.0019, 0.0026, 0.0029, 0.0020,\n",
      "        0.0032, 0.0021, 0.0023, 0.0025, 0.0018, 0.0018, 0.0025, 0.0018, 0.0024,\n",
      "        0.0024, 0.0020, 0.0025, 0.0020, 0.0032, 0.0022, 0.0017, 0.0022, 0.0016,\n",
      "        0.0018, 0.0018, 0.0018, 0.0045, 0.0018, 0.0017, 0.0025, 0.0024, 0.0023,\n",
      "        0.0028, 0.0021, 0.0023, 0.0015, 0.0026, 0.0033, 0.0014, 0.0024, 0.0032,\n",
      "        0.0020, 0.0022, 0.0018, 0.0028, 0.0021, 0.0058, 0.0018, 0.0024, 0.0042,\n",
      "        0.0021, 0.0017, 0.0019, 0.0021, 0.0023, 0.0027, 0.0017, 0.0019, 0.0022,\n",
      "        0.0018, 0.0020, 0.0020, 0.0023, 0.0032, 0.0020, 0.0023, 0.0022, 0.0022,\n",
      "        0.0022, 0.0021, 0.0019, 0.0017, 0.0027, 0.0017, 0.0015, 0.0018, 0.0017,\n",
      "        0.0020, 0.0024, 0.0023, 0.0028, 0.0023, 0.0019, 0.0023, 0.0029, 0.0021,\n",
      "        0.0023, 0.0017, 0.0019, 0.0024, 0.0029, 0.0020, 0.0017, 0.0018, 0.0020,\n",
      "        0.0021, 0.0025, 0.0023, 0.0019, 0.0016, 0.0019, 0.0018, 0.0023, 0.0021,\n",
      "        0.0016, 0.0024, 0.0022, 0.0026, 0.0022, 0.0019, 0.0024, 0.0017, 0.0019,\n",
      "        0.0018, 0.0019, 0.0021, 0.0022, 0.0028, 0.0016, 0.0018, 0.0023, 0.0020,\n",
      "        0.0034, 0.0021, 0.0018, 0.0020, 0.0018, 0.0020, 0.0021, 0.0016, 0.0017,\n",
      "        0.0016, 0.0020, 0.0020, 0.0023, 0.0022, 0.0023, 0.0021, 0.0036, 0.0020,\n",
      "        0.0031, 0.0022, 0.0018, 0.0021, 0.0022, 0.0020, 0.0021, 0.0016, 0.0023,\n",
      "        0.0022, 0.0027, 0.0018, 0.0018, 0.0023, 0.0034, 0.0017, 0.0020, 0.0023,\n",
      "        0.0021, 0.0025, 0.0028, 0.0021, 0.0026, 0.0021, 0.0020, 0.0018, 0.0033,\n",
      "        0.0019, 0.0023, 0.0017, 0.0017, 0.0020, 0.0019, 0.0026, 0.0024, 0.0020,\n",
      "        0.0023, 0.0021, 0.0023, 0.0026, 0.0019, 0.0019, 0.0020, 0.0019, 0.0023,\n",
      "        0.0019, 0.0019, 0.0021, 0.0020, 0.0020, 0.0021, 0.0020, 0.0016, 0.0018,\n",
      "        0.0019, 0.0016, 0.0019, 0.0017, 0.0036, 0.0017, 0.0021, 0.0020, 0.0018,\n",
      "        0.0024, 0.0023, 0.0018, 0.0023, 0.0029, 0.0027, 0.0016, 0.0022, 0.0024,\n",
      "        0.0019, 0.0021, 0.0025, 0.0020, 0.0020, 0.0046, 0.0024, 0.0019, 0.0030,\n",
      "        0.0020, 0.0024, 0.0026, 0.0027, 0.0024, 0.0024, 0.0030, 0.0039, 0.0021,\n",
      "        0.0019, 0.0027, 0.0032, 0.0022, 0.0019, 0.0024, 0.0021, 0.0018, 0.0021,\n",
      "        0.0021, 0.0026, 0.0025, 0.0015, 0.0020, 0.0023, 0.0019, 0.0028, 0.0018,\n",
      "        0.0034, 0.0018, 0.0021, 0.0018, 0.0022, 0.0029, 0.0023, 0.0024, 0.0023,\n",
      "        0.0019, 0.0021, 0.0018, 0.0027, 0.0019, 0.0027, 0.0023, 0.0019, 0.0019,\n",
      "        0.0028, 0.0018, 0.0016, 0.0029, 0.0022, 0.0021, 0.0033, 0.0017, 0.0038,\n",
      "        0.0038, 0.0021, 0.0019, 0.0021, 0.0019, 0.0027, 0.0032, 0.0018, 0.0019,\n",
      "        0.0020, 0.0020, 0.0015, 0.0020, 0.0017, 0.0039, 0.0032, 0.0033, 0.0033,\n",
      "        0.0021, 0.0024, 0.0019, 0.0047, 0.0027, 0.0025, 0.0023, 0.0033, 0.0032,\n",
      "        0.0020, 0.0025, 0.0022, 0.0022, 0.0020, 0.0017, 0.0019, 0.0024, 0.0019,\n",
      "        0.0019, 0.0022, 0.0030, 0.0034, 0.0025, 0.0036, 0.0023, 0.0020, 0.0025,\n",
      "        0.0016, 0.0019, 0.0019, 0.0038, 0.0018, 0.0018, 0.0020, 0.0023, 0.0023,\n",
      "        0.0026, 0.0023, 0.0016, 0.0025, 0.0024, 0.0028, 0.0027, 0.0017, 0.0015,\n",
      "        0.0022, 0.0020, 0.0018, 0.0033, 0.0024, 0.0029, 0.0023, 0.0029, 0.0023,\n",
      "        0.0022, 0.0025, 0.0025, 0.0031, 0.0025, 0.0022, 0.0025, 0.0031, 0.0022,\n",
      "        0.0020, 0.0018, 0.0022, 0.0025, 0.0025, 0.0019, 0.0016, 0.0027, 0.0018,\n",
      "        0.0018, 0.0020, 0.0021, 0.0022, 0.0019, 0.0026, 0.0021, 0.0019, 0.0018,\n",
      "        0.0027, 0.0028, 0.0025, 0.0025, 0.0022, 0.0026, 0.0025, 0.0022, 0.0016,\n",
      "        0.0019, 0.0021, 0.0021, 0.0026, 0.0022, 0.0034, 0.0030, 0.0024, 0.0027,\n",
      "        0.0035, 0.0024, 0.0023, 0.0028, 0.0024, 0.0016, 0.0033, 0.0023, 0.0023,\n",
      "        0.0018, 0.0021, 0.0036, 0.0024, 0.0017, 0.0016, 0.0022, 0.0024, 0.0024,\n",
      "        0.0025, 0.0023, 0.0028, 0.0017, 0.0025, 0.0021, 0.0020, 0.0019, 0.0019,\n",
      "        0.0024, 0.0022, 0.0020, 0.0028, 0.0020, 0.0023, 0.0025, 0.0021, 0.0023,\n",
      "        0.0019, 0.0024, 0.0024, 0.0022, 0.0023, 0.0027, 0.0026, 0.0020, 0.0029,\n",
      "        0.0032], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.0540e-03, -4.1489e-03, -2.0516e-02, -1.7642e-02,  9.6078e-03,\n",
      "         5.8760e-03, -1.4448e-02,  7.1637e-04,  1.4057e-03, -1.6386e-03,\n",
      "        -1.2073e-02, -9.3700e-03, -1.9250e-02, -1.7166e-02, -1.1226e-02,\n",
      "        -1.6972e-02, -8.1289e-03, -1.3626e-02,  8.5068e-03, -1.7855e-02,\n",
      "         6.9540e-03,  1.8412e-02, -2.4812e-03,  2.4328e-03, -1.3534e-03,\n",
      "         4.8874e-03,  1.0521e-02, -8.0455e-04, -6.4342e-03, -1.1446e-02,\n",
      "        -1.3326e-02, -9.2675e-03, -2.5020e-03, -4.5003e-03,  3.7125e-03,\n",
      "        -2.4581e-02,  4.4222e-03, -8.4804e-03, -8.9190e-06, -1.7266e-03,\n",
      "        -4.1818e-03,  2.4226e-03,  9.5379e-03, -6.6336e-03,  2.2586e-03,\n",
      "         3.6322e-04,  2.7801e-02, -1.1270e-02, -1.5046e-02, -8.3841e-03,\n",
      "         5.7193e-03, -1.5327e-02,  8.8598e-04,  2.3715e-02,  1.3557e-02,\n",
      "         6.5433e-03,  1.3621e-02, -9.8292e-03,  7.2298e-03,  1.7554e-02,\n",
      "         2.0959e-04,  4.3992e-03, -8.7419e-03,  9.7137e-03,  1.1456e-02,\n",
      "         1.5336e-02, -6.9862e-03, -1.3379e-03,  5.0565e-03, -1.4394e-03,\n",
      "        -2.0059e-02,  6.6472e-03,  7.4694e-04, -7.4046e-04,  3.9481e-03,\n",
      "         1.8126e-03,  2.2948e-03,  5.0710e-03,  4.2257e-03,  6.9578e-03,\n",
      "        -1.4057e-02,  1.2723e-02,  5.3173e-03, -3.3818e-02, -1.4235e-02,\n",
      "        -3.0357e-03, -1.1859e-02,  1.1082e-03, -5.6086e-04,  8.8305e-03,\n",
      "        -6.6599e-03, -1.0402e-02,  8.7649e-03, -3.4985e-03, -1.0276e-02,\n",
      "        -1.9184e-02, -1.6150e-03, -1.0940e-02, -1.7612e-02, -5.9452e-05,\n",
      "        -6.8167e-03, -1.5879e-02, -2.0135e-03,  7.3849e-03,  4.6699e-03,\n",
      "        -1.1819e-02, -2.2258e-03, -1.0042e-02, -1.4626e-02, -2.9059e-02,\n",
      "        -1.9744e-02,  2.1017e-02, -4.9493e-03, -1.4729e-02, -5.7178e-03,\n",
      "        -2.7879e-02, -1.1342e-02, -1.1067e-02,  3.6126e-03, -1.6722e-03,\n",
      "        -5.8225e-03, -1.1147e-02, -5.8773e-03, -1.3853e-02,  7.6938e-03,\n",
      "        -3.1506e-04, -1.1560e-02,  1.6636e-03, -8.3648e-03, -1.7294e-03,\n",
      "        -1.3527e-02,  1.2758e-03,  3.0125e-03,  2.0911e-03, -1.6461e-03,\n",
      "        -1.9466e-02, -8.0977e-03, -1.3765e-02,  4.7957e-03, -1.8019e-02,\n",
      "        -1.7326e-02, -3.8022e-03, -2.5910e-02, -4.7377e-03, -1.1454e-02,\n",
      "         1.9997e-03,  1.3955e-03, -2.2347e-02, -9.7723e-03, -1.5300e-02,\n",
      "        -1.3231e-03,  8.4604e-03, -1.9220e-02,  1.1905e-02, -4.7311e-04,\n",
      "        -3.2012e-03, -2.0542e-03, -1.1135e-02, -6.5770e-03, -3.0687e-03,\n",
      "        -7.1176e-04,  9.2847e-04,  3.3504e-02,  1.1147e-02, -2.3674e-03,\n",
      "        -6.1488e-03, -2.9458e-03, -1.9080e-02,  7.8941e-04, -7.8993e-03,\n",
      "         9.5198e-03,  1.6350e-02,  8.9761e-03,  8.0201e-03,  9.6033e-03,\n",
      "        -1.1656e-02,  6.9022e-03, -7.3137e-03,  1.3641e-02,  5.3347e-03,\n",
      "         3.9581e-03, -6.1368e-03,  7.6379e-03, -1.1373e-02,  7.7446e-03,\n",
      "         9.7012e-03, -2.3150e-03,  9.7793e-03,  3.0096e-03,  4.5153e-03,\n",
      "        -7.5008e-03,  9.4292e-03,  2.2289e-02, -1.6589e-02, -6.2875e-03,\n",
      "         2.1770e-02,  1.1185e-02,  7.9690e-03, -2.3495e-02,  6.2308e-03,\n",
      "        -1.7196e-02,  8.5162e-03,  1.8709e-02,  1.8687e-02,  1.4016e-02,\n",
      "        -3.9788e-04, -2.1075e-02,  3.5216e-03, -4.6406e-03,  1.9511e-02,\n",
      "        -1.0128e-02,  8.2909e-03, -1.3885e-03, -1.0802e-02,  2.1202e-03,\n",
      "        -3.6062e-04, -7.9470e-03,  2.5781e-02,  3.0053e-03,  3.5437e-03,\n",
      "         2.1905e-03, -1.2686e-02, -1.7843e-03,  2.8698e-02, -1.6651e-02,\n",
      "         1.9534e-02, -1.8055e-03,  6.1631e-03, -8.6250e-03,  4.9767e-03,\n",
      "         5.5962e-04, -4.8662e-03,  1.8979e-02,  4.0372e-04,  7.4906e-03,\n",
      "        -3.6779e-03,  2.4809e-02,  6.1266e-03,  7.1814e-03,  2.3046e-02,\n",
      "        -5.0567e-03, -2.6188e-03,  3.8702e-03, -4.8383e-03,  2.9464e-03,\n",
      "         1.6378e-02,  2.4133e-03,  6.3303e-03,  1.1527e-03, -6.9009e-03,\n",
      "         8.5641e-03,  1.1656e-02, -3.9624e-04, -1.5257e-02,  1.2500e-02,\n",
      "        -3.1723e-03,  9.7659e-03,  2.0472e-03,  1.3458e-02,  3.1983e-03,\n",
      "         6.1478e-03, -5.5542e-03,  4.5304e-03,  9.5399e-03, -8.0610e-04,\n",
      "        -2.6285e-03,  9.3042e-03, -2.6574e-03, -1.7429e-02,  4.1200e-03,\n",
      "         2.1546e-02, -6.7119e-03,  3.5949e-03,  1.0285e-02, -8.3183e-03,\n",
      "        -1.3146e-02,  5.1571e-03, -4.9292e-03, -5.1749e-03,  3.9995e-03,\n",
      "        -1.1157e-02,  2.3007e-02, -1.3237e-02, -1.3539e-02,  1.6401e-02,\n",
      "         1.8551e-02, -6.2334e-03, -4.0546e-03, -5.4428e-03, -8.5930e-03,\n",
      "        -9.7738e-03, -1.7880e-02, -4.2471e-03, -8.9907e-03, -4.4864e-03,\n",
      "        -3.3912e-03, -9.0068e-03, -7.6528e-03,  1.4259e-04,  1.2927e-02,\n",
      "        -1.0614e-02, -3.2305e-03, -7.8824e-03, -3.4433e-03, -7.5388e-03,\n",
      "         1.0577e-02,  1.6554e-02, -8.3268e-04, -1.3322e-02, -5.6734e-03,\n",
      "         4.8978e-03,  3.2308e-03,  4.4596e-03,  2.1594e-02,  2.2659e-02,\n",
      "         6.1472e-03,  5.5816e-03, -3.1229e-03,  9.6828e-03, -3.1044e-03,\n",
      "        -2.2434e-02, -1.8109e-02, -2.5078e-02, -1.4465e-02, -2.1683e-02,\n",
      "        -2.1719e-02, -2.2218e-02, -4.0805e-03, -6.8154e-03, -4.6295e-04,\n",
      "         2.1050e-03, -1.9103e-03, -2.2263e-04,  3.1099e-03, -3.9733e-03,\n",
      "        -1.5902e-02,  1.4425e-02, -1.6939e-02, -8.6551e-03, -9.6204e-03,\n",
      "        -8.7589e-03, -1.0852e-02,  8.5307e-04, -1.8470e-03, -7.0786e-03,\n",
      "         6.6244e-04,  6.5149e-03, -9.2485e-03,  1.3138e-02, -9.2627e-03,\n",
      "         1.2277e-02, -6.7726e-03, -4.5025e-03, -2.4775e-03, -3.2238e-03,\n",
      "        -6.2376e-03, -3.1175e-05, -8.4149e-03,  1.1946e-02,  1.2635e-02,\n",
      "         9.7893e-03,  1.1308e-02, -5.7688e-04, -2.0763e-03, -4.9488e-03,\n",
      "         8.9936e-04, -1.4518e-02,  9.4824e-03, -8.2777e-03,  7.6667e-03,\n",
      "        -7.6525e-03, -5.0149e-03, -2.0475e-03,  3.6663e-03, -4.1791e-03,\n",
      "        -7.1726e-03, -7.5156e-04, -1.2771e-02,  1.0262e-02,  9.8359e-03,\n",
      "        -1.6181e-02,  2.4103e-03,  2.2923e-03,  4.0631e-03, -2.1448e-02,\n",
      "        -7.8552e-03,  2.7749e-03, -7.1327e-03, -1.8056e-02, -4.4744e-03,\n",
      "        -9.6018e-03,  1.4480e-02, -1.8536e-02, -2.3572e-02, -9.6817e-03,\n",
      "        -9.1566e-03, -2.3145e-02, -1.1013e-02, -1.0774e-02,  4.0369e-03,\n",
      "        -1.5529e-02,  1.1558e-02,  2.9494e-03,  4.5030e-03, -2.6361e-02,\n",
      "         1.6857e-02, -6.7117e-04, -8.6142e-03, -1.5695e-02,  2.5245e-04,\n",
      "        -1.0122e-03, -6.8395e-03,  4.6690e-03,  1.8952e-02,  1.5193e-02,\n",
      "        -2.3246e-03,  3.8103e-03, -9.5545e-03,  6.1614e-03,  7.6473e-03,\n",
      "        -2.8300e-03,  4.8893e-04,  8.8167e-03,  1.7012e-02,  1.5419e-02,\n",
      "        -8.8604e-05, -2.6620e-02, -1.1690e-02,  1.1404e-02,  4.8491e-03,\n",
      "        -6.3126e-03,  7.5267e-03, -5.7180e-03,  1.3621e-02,  6.8829e-03,\n",
      "        -4.1245e-03, -8.8954e-04,  3.1794e-03,  3.3397e-03, -9.8891e-03,\n",
      "         9.5482e-03,  1.3731e-03,  3.3720e-03, -3.5529e-03,  1.0321e-02,\n",
      "         1.9340e-02,  1.2044e-02, -2.5388e-03,  2.2359e-02,  5.6450e-03,\n",
      "         4.2535e-03, -4.7033e-03, -4.3303e-03, -4.9201e-03,  4.0193e-03,\n",
      "         7.9716e-04,  1.4354e-02,  1.0510e-02, -1.6068e-02,  1.0776e-02,\n",
      "         2.4135e-03, -3.1489e-03,  4.0862e-03,  1.5338e-02,  2.7293e-02,\n",
      "        -6.6519e-03, -8.8138e-03,  2.0641e-03,  1.0558e-02, -1.1751e-03,\n",
      "         2.6450e-02, -4.5885e-03, -2.5581e-03,  2.2879e-03, -1.6937e-02,\n",
      "        -1.6884e-02, -2.2681e-02,  5.2053e-04, -1.3454e-02,  1.3260e-03,\n",
      "         8.3329e-03, -3.0078e-03,  1.6696e-03, -4.7768e-03,  1.3149e-03,\n",
      "        -3.2169e-03,  2.9908e-03,  2.5137e-02,  6.4319e-03,  2.6557e-03,\n",
      "        -2.0985e-02,  5.1428e-03,  3.7225e-03, -1.8776e-03,  9.8686e-04,\n",
      "        -7.6931e-03, -4.7424e-03, -1.0604e-02, -4.8097e-03, -3.3184e-03,\n",
      "        -1.8420e-02, -1.3153e-02,  7.3985e-03, -2.7261e-04, -1.7159e-03,\n",
      "        -1.3054e-03,  7.0221e-03,  2.1518e-04,  7.5642e-03, -1.6226e-02,\n",
      "        -1.0975e-02,  7.3913e-03,  1.3499e-03,  3.4619e-03, -1.0177e-02,\n",
      "         2.1055e-02, -5.6968e-03, -5.8522e-03, -2.8396e-03,  1.5228e-03,\n",
      "        -1.6070e-02, -4.4185e-03,  1.0954e-02,  1.5877e-02, -1.0259e-02,\n",
      "        -2.3040e-02,  1.4476e-02,  1.3044e-02, -3.5107e-03,  1.4101e-02,\n",
      "         1.4979e-02, -4.6421e-03,  1.1947e-02,  1.9238e-03,  8.4225e-03,\n",
      "         6.4436e-03,  1.7598e-03,  1.1681e-02,  9.4544e-03,  1.9474e-02,\n",
      "        -2.6913e-02,  4.3572e-03,  6.4584e-03, -1.1189e-03,  7.1997e-03,\n",
      "         2.5116e-03, -2.7444e-04, -1.2989e-02, -9.0542e-03, -5.0313e-03,\n",
      "         3.0390e-03,  2.0017e-03,  4.2103e-05,  1.5548e-03, -7.3574e-03,\n",
      "        -1.8541e-03, -1.9903e-02,  2.1203e-02,  8.1240e-03,  9.4240e-03,\n",
      "        -6.1419e-03, -3.2745e-03, -6.8238e-03,  1.2302e-02, -1.2866e-02,\n",
      "        -4.0878e-03, -9.0624e-03,  2.0396e-02, -7.1515e-03, -5.0075e-03,\n",
      "        -1.8688e-03,  5.4391e-03,  7.7934e-03, -6.8209e-03,  1.8686e-03,\n",
      "         2.0032e-03,  4.4714e-04,  2.9370e-03, -5.6603e-04,  9.7942e-03,\n",
      "        -2.4189e-03, -2.7806e-03,  1.6427e-02,  1.3859e-02,  1.6110e-02,\n",
      "        -6.9551e-04, -2.0631e-02,  1.3816e-02,  7.7730e-04, -5.9685e-04,\n",
      "        -6.0041e-03, -2.6906e-03, -6.7904e-03,  1.8637e-02, -3.1624e-03,\n",
      "        -6.1068e-03, -5.3474e-03,  1.0279e-02, -8.5100e-03, -3.9488e-03,\n",
      "         1.1338e-02,  1.3868e-02,  3.8278e-03, -4.0237e-03, -1.0225e-02,\n",
      "         4.0387e-03,  2.6453e-03,  2.6120e-03,  8.6405e-03, -1.1117e-02,\n",
      "         1.4388e-02, -1.4128e-02, -1.2018e-02, -1.0592e-02, -6.6603e-03,\n",
      "         7.9171e-03,  4.0072e-03, -7.6342e-03,  1.6426e-02, -1.0826e-02,\n",
      "         1.0751e-02,  8.9987e-03,  1.3392e-02,  3.4955e-02,  1.2790e-02,\n",
      "         6.5807e-03,  1.7475e-02,  9.3045e-03, -4.9896e-03,  1.4937e-02,\n",
      "         1.5019e-04, -1.1449e-02,  1.7550e-02, -6.4155e-03, -7.5740e-03,\n",
      "        -7.9086e-03,  7.7305e-03, -4.9878e-03,  4.6029e-03, -1.3565e-02,\n",
      "        -1.9914e-02,  1.3758e-02,  5.6335e-03,  2.4345e-03,  2.7075e-02,\n",
      "        -8.6951e-03,  2.6696e-02, -1.8396e-02,  6.7324e-03,  8.8317e-03,\n",
      "         3.7132e-02,  1.7988e-03,  5.6044e-03,  5.8572e-03,  1.1837e-02,\n",
      "         2.2514e-02,  1.2283e-03,  2.3551e-03, -8.6822e-03,  1.1198e-02,\n",
      "         6.2164e-04, -7.5180e-03,  4.8814e-03,  1.9022e-03,  8.9163e-03,\n",
      "         1.7953e-02,  7.1988e-03,  3.7652e-03, -2.0105e-02, -1.9763e-02,\n",
      "         3.2020e-03,  1.8837e-02,  1.2321e-02,  1.3820e-02,  4.1368e-03,\n",
      "        -1.6280e-02, -2.9063e-03,  2.7845e-02,  6.1211e-03,  5.3014e-03,\n",
      "         7.0098e-03,  2.2649e-02,  1.1360e-03,  2.9727e-02,  1.3664e-02,\n",
      "        -6.8027e-03,  1.4364e-02, -5.5152e-03, -1.4049e-02, -1.0417e-02,\n",
      "        -2.3374e-02, -4.5921e-03,  2.1876e-02, -9.0628e-03, -6.1995e-03,\n",
      "         1.2690e-02,  8.1973e-03, -1.0950e-02, -5.6462e-03, -4.7800e-04,\n",
      "         2.3818e-02,  9.5816e-04, -4.7822e-03,  8.7511e-03,  1.4342e-02,\n",
      "        -1.6386e-02, -9.0176e-04,  4.6307e-04, -2.2330e-03, -1.1017e-02,\n",
      "        -7.3926e-03,  8.6733e-03, -9.6236e-03,  5.8556e-03,  6.5153e-03,\n",
      "        -7.7699e-03,  2.1073e-02, -9.3213e-03,  1.3332e-02,  1.6535e-02,\n",
      "         5.1240e-03, -3.4339e-03,  2.4813e-02, -6.0891e-03, -8.6840e-03,\n",
      "         2.1645e-03, -3.5292e-03, -1.2985e-02, -9.5567e-03, -3.7438e-03,\n",
      "         3.2002e-03,  1.5815e-02,  8.0564e-03,  7.5541e-03, -1.0808e-02,\n",
      "        -6.4124e-04, -1.4614e-03,  3.7593e-03, -3.1028e-03,  1.0774e-02,\n",
      "        -4.6029e-03, -9.1495e-03,  8.3655e-04,  9.2427e-03,  1.3866e-02,\n",
      "         5.6882e-03, -3.3533e-03,  2.7270e-04, -4.6015e-03,  1.9522e-02,\n",
      "         4.0074e-03, -6.9705e-03,  1.3168e-02,  2.0821e-03,  9.5050e-03,\n",
      "        -2.3545e-03,  6.2672e-03, -1.2549e-02,  8.4020e-03,  7.7575e-03,\n",
      "         2.6701e-03,  9.9543e-03, -1.1701e-02,  1.6242e-03,  8.8834e-03,\n",
      "         4.9853e-03, -1.3091e-02, -6.0042e-03,  4.7919e-03,  8.4710e-03,\n",
      "        -3.4157e-03, -1.1133e-02,  1.2986e-02,  5.0915e-03,  2.0220e-02,\n",
      "        -1.7791e-03,  9.3522e-03,  1.9787e-02, -2.5497e-03, -1.0211e-02,\n",
      "        -1.3922e-02, -4.6255e-03,  3.8947e-03,  1.3278e-03,  6.0785e-03,\n",
      "         6.1298e-03, -2.9151e-03, -6.8150e-03,  5.6859e-03,  1.0645e-02,\n",
      "        -1.6793e-02,  1.6135e-02,  1.9848e-03,  5.5552e-03, -3.0141e-03,\n",
      "        -1.2028e-04, -2.6739e-03, -7.8973e-04, -1.2256e-02,  1.1130e-02,\n",
      "        -2.1308e-02, -1.5121e-03, -8.4693e-03, -9.0220e-03, -1.5707e-03,\n",
      "         2.1435e-02, -1.4622e-02, -1.1779e-02,  3.0766e-03, -1.7704e-03,\n",
      "        -1.3916e-02, -7.2216e-03, -1.0567e-03, -9.6687e-04, -4.2216e-03,\n",
      "        -1.2833e-02, -2.7412e-04,  6.6940e-04, -4.7578e-03, -5.6300e-03,\n",
      "        -1.2190e-02, -4.4394e-03,  5.1390e-03,  1.3539e-02,  9.9249e-03,\n",
      "         1.4757e-02,  2.6018e-03, -4.3809e-03,  2.4684e-02, -1.1273e-02,\n",
      "         1.2729e-02, -5.1084e-03, -1.1814e-02, -4.9827e-03,  1.4937e-02,\n",
      "        -1.7126e-02,  1.6598e-02,  1.3946e-02,  3.0377e-02,  6.6425e-03,\n",
      "         6.0981e-03, -3.3217e-03,  8.1140e-03, -1.6228e-03,  1.9549e-02,\n",
      "         2.6386e-02, -8.1902e-04,  2.5420e-03,  7.9035e-03,  9.1169e-03,\n",
      "         1.6212e-02, -4.3473e-03,  1.0761e-02,  1.7077e-03, -1.4481e-02,\n",
      "        -5.2910e-03, -2.2974e-02, -2.0641e-02,  1.0192e-02,  1.0545e-02,\n",
      "        -9.7636e-03,  1.8227e-02, -8.2728e-03,  8.9075e-03, -2.2620e-03,\n",
      "        -9.6952e-03, -5.0159e-03,  7.1226e-03,  2.8585e-02, -8.3582e-05,\n",
      "         2.1185e-03, -7.7900e-03,  1.7819e-02, -8.3359e-03, -2.5210e-02,\n",
      "         5.0447e-03,  1.5186e-03,  4.7860e-03, -1.5937e-02,  2.5210e-02,\n",
      "         1.7711e-02,  1.0091e-03,  3.7658e-04, -1.9004e-04,  2.4856e-03,\n",
      "         3.5618e-02, -1.4573e-02, -1.6398e-02,  3.4927e-03,  2.5905e-03,\n",
      "        -5.4803e-04, -5.2864e-03,  1.7263e-02, -8.6483e-04,  1.9328e-04,\n",
      "         1.3726e-02,  6.9260e-03,  1.4363e-02,  2.0397e-02, -8.4448e-03,\n",
      "        -5.5349e-03, -1.8207e-03,  2.2978e-02,  1.9817e-02, -1.5055e-02,\n",
      "        -6.0768e-03,  1.8341e-02,  1.4490e-02, -1.8010e-03,  1.1170e-02,\n",
      "         1.0210e-02,  1.8254e-02, -8.1524e-03,  1.0713e-02, -5.9407e-03,\n",
      "        -6.2752e-03, -2.9218e-02, -1.6438e-02, -3.8385e-03,  2.4974e-02,\n",
      "         1.7872e-02, -2.0053e-02, -1.8510e-02,  9.9373e-04, -1.9877e-02,\n",
      "        -6.1206e-03, -1.2718e-02, -8.8157e-03, -1.1485e-02,  2.1413e-02,\n",
      "         9.8183e-03, -8.0588e-03, -2.4606e-03, -1.0453e-02,  6.4749e-03,\n",
      "        -1.2692e-02, -1.2120e-03, -8.2088e-03, -1.5537e-02, -1.3258e-02,\n",
      "         4.4749e-03, -4.2462e-03, -1.3773e-02, -1.8466e-02, -7.4500e-03,\n",
      "         3.2231e-03, -2.2158e-03, -7.6571e-03, -2.4689e-03, -1.5317e-03,\n",
      "         2.6943e-03,  7.9552e-04, -3.7696e-03, -1.4036e-02, -2.6654e-04,\n",
      "        -6.5476e-03, -1.2888e-04, -1.9731e-04, -1.4204e-04, -1.9490e-02,\n",
      "        -1.1283e-03,  3.8485e-03, -1.3964e-02, -9.2690e-03, -1.8712e-02,\n",
      "        -9.7369e-04,  2.2355e-02,  2.0221e-02,  7.1197e-03,  9.9006e-03,\n",
      "        -1.1104e-02,  5.4583e-03,  9.0527e-03, -4.2292e-03, -3.0734e-05,\n",
      "         9.2553e-03,  1.4336e-02,  2.5846e-02,  5.2189e-03, -1.1753e-02,\n",
      "         1.8853e-02, -6.9766e-03,  1.0509e-02, -1.1962e-02,  7.0652e-03,\n",
      "         1.0629e-03, -1.6302e-02, -1.9871e-03, -7.4473e-03,  1.7128e-03,\n",
      "         3.0291e-03, -1.2221e-02, -1.4638e-02, -3.3854e-02, -1.1569e-02,\n",
      "        -1.6404e-02, -4.0354e-03, -1.3008e-02,  7.7703e-03,  2.4369e-03],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# SAVE FC PARAM\n",
    "# model_fp32.fc.scale\n",
    "# model_fp32.fc.zero_point\n",
    "# model_fp32.fc._packed_params.dtype\n",
    "# model_fp32.fc._packed_params._packed_params\n",
    "layername = 'model_fp32.fc.'\n",
    "\n",
    "print(quantized_model.state_dict()[layername + 'scale'])\n",
    "print(quantized_model.state_dict()[layername + 'zero_point'])\n",
    "print(quantized_model.state_dict()[layername + '_packed_params.dtype'])\n",
    "\n",
    "fcweight = quantized_model.state_dict()[layername + '_packed_params._packed_params'][0]\n",
    "print(fcweight)\n",
    "\n",
    "fcweight_int = quantized_model.state_dict()[layername + '_packed_params._packed_params'][0].int_repr()\n",
    "print(fcweight_int)\n",
    "\n",
    "fcweight_scale = quantized_model.state_dict()[layername + '_packed_params._packed_params'][0].q_per_channel_scales()\n",
    "print(fcweight_scale)\n",
    "\n",
    "fcbias = quantized_model.state_dict()[layername + '_packed_params._packed_params'][1]\n",
    "print(fcbias)\n",
    "\n",
    "np.savetxt('./resnet50int8-param/fc.weight.txt', np.ravel(np.array(fcweight_int, dtype='int8')))\n",
    "np.savetxt('./resnet50int8-param/fc.scale.txt', np.ravel(np.array(fcweight_scale, dtype='float32')))\n",
    "np.savetxt('./resnet50int8-param/fc.bias.txt', np.ravel(fcbias.to('cpu').detach().numpy().copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excess-portfolio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./resnet50int8-param/layer1.0.downsample.0.weight.txt\n",
      "./resnet50int8-param/layer1.0.conv1.weight.txt\n",
      "./resnet50int8-param/layer1.0.conv2.weight.txt\n",
      "./resnet50int8-param/layer1.0.conv3.weight.txt\n",
      "./resnet50int8-param/layer1.1.conv1.weight.txt\n",
      "./resnet50int8-param/layer1.1.conv2.weight.txt\n",
      "./resnet50int8-param/layer1.1.conv3.weight.txt\n",
      "./resnet50int8-param/layer1.2.conv1.weight.txt\n",
      "./resnet50int8-param/layer1.2.conv2.weight.txt\n",
      "./resnet50int8-param/layer1.2.conv3.weight.txt\n",
      "./resnet50int8-param/layer2.0.downsample.0.weight.txt\n",
      "./resnet50int8-param/layer2.0.conv1.weight.txt\n",
      "./resnet50int8-param/layer2.0.conv2.weight.txt\n",
      "./resnet50int8-param/layer2.0.conv3.weight.txt\n",
      "./resnet50int8-param/layer2.1.conv1.weight.txt\n",
      "./resnet50int8-param/layer2.1.conv2.weight.txt\n",
      "./resnet50int8-param/layer2.1.conv3.weight.txt\n",
      "./resnet50int8-param/layer2.2.conv1.weight.txt\n",
      "./resnet50int8-param/layer2.2.conv2.weight.txt\n",
      "./resnet50int8-param/layer2.2.conv3.weight.txt\n",
      "./resnet50int8-param/layer2.3.conv1.weight.txt\n",
      "./resnet50int8-param/layer2.3.conv2.weight.txt\n",
      "./resnet50int8-param/layer2.3.conv3.weight.txt\n",
      "./resnet50int8-param/layer3.0.downsample.0.weight.txt\n",
      "./resnet50int8-param/layer3.0.conv1.weight.txt\n",
      "./resnet50int8-param/layer3.0.conv2.weight.txt\n",
      "./resnet50int8-param/layer3.0.conv3.weight.txt\n",
      "./resnet50int8-param/layer3.1.conv1.weight.txt\n",
      "./resnet50int8-param/layer3.1.conv2.weight.txt\n",
      "./resnet50int8-param/layer3.1.conv3.weight.txt\n",
      "./resnet50int8-param/layer3.2.conv1.weight.txt\n",
      "./resnet50int8-param/layer3.2.conv2.weight.txt\n",
      "./resnet50int8-param/layer3.2.conv3.weight.txt\n",
      "./resnet50int8-param/layer3.3.conv1.weight.txt\n",
      "./resnet50int8-param/layer3.3.conv2.weight.txt\n",
      "./resnet50int8-param/layer3.3.conv3.weight.txt\n",
      "./resnet50int8-param/layer3.4.conv1.weight.txt\n",
      "./resnet50int8-param/layer3.4.conv2.weight.txt\n",
      "./resnet50int8-param/layer3.4.conv3.weight.txt\n",
      "./resnet50int8-param/layer3.5.conv1.weight.txt\n",
      "./resnet50int8-param/layer3.5.conv2.weight.txt\n",
      "./resnet50int8-param/layer3.5.conv3.weight.txt\n",
      "./resnet50int8-param/layer4.0.downsample.0.weight.txt\n",
      "./resnet50int8-param/layer4.0.conv1.weight.txt\n",
      "./resnet50int8-param/layer4.0.conv2.weight.txt\n",
      "./resnet50int8-param/layer4.0.conv3.weight.txt\n",
      "./resnet50int8-param/layer4.1.conv1.weight.txt\n",
      "./resnet50int8-param/layer4.1.conv2.weight.txt\n",
      "./resnet50int8-param/layer4.1.conv3.weight.txt\n",
      "./resnet50int8-param/layer4.2.conv1.weight.txt\n",
      "./resnet50int8-param/layer4.2.conv2.weight.txt\n",
      "./resnet50int8-param/layer4.2.conv3.weight.txt\n",
      "DDR SIZE\n",
      "ddr0:  37888\n",
      "ddr1:  992000\n",
      "ddr2:  2643968\n",
      "ddr3:  2754512\n",
      "total: 6428368\n"
     ]
    }
   ],
   "source": [
    "# MAKE DDR FILE\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "TOO = 64\n",
    "TII = 32\n",
    "\n",
    "def read_params(w_fname, b_fname, s_fname, OCH, ICH, K, ddr, ISCALE, OSCALE):\n",
    "    global bsfile\n",
    "    print(w_fname)\n",
    "    tmp = []\n",
    "    array = []\n",
    "    \n",
    "    for line in open(w_fname).readlines():\n",
    "        tmp.append(int(float(line)))\n",
    "    \n",
    "    for to in range(OCH // TOO):\n",
    "        for ti in range(ICH // TII):\n",
    "            for i in range(K):\n",
    "                for j in range(K):\n",
    "                    for too in range(TOO // 2):\n",
    "                        for tii in range(TII):\n",
    "                            array.append(tmp[(to*TOO + 2*too  )*ICH*K*K + (ti*TII + tii)*K*K + i*K + j])\n",
    "                            array.append(tmp[(to*TOO + 2*too+1)*ICH*K*K + (ti*TII + tii)*K*K + i*K + j])\n",
    "    \n",
    "    for i in range(len(array) // 4):\n",
    "        b0 = format(array[4*i    ] & 0xff, '08b')\n",
    "        b1 = format(array[4*i + 1] & 0xff, '08b')\n",
    "        b2 = format(array[4*i + 2] & 0xff, '08b')\n",
    "        b3 = format(array[4*i + 3] & 0xff, '08b')\n",
    "        bdata = b3 + b2 + b1 + b0\n",
    "        idata = int(bdata, 2)\n",
    "        ddr.append(idata)\n",
    "    \n",
    "    for line in open(b_fname).readlines():\n",
    "        data = float(line) / OSCALE\n",
    "        bdata = struct.pack('>f', data)\n",
    "        idata = int.from_bytes(bdata,'big')\n",
    "        ddr.append(idata)\n",
    "            \n",
    "    for line in open(s_fname).readlines():\n",
    "        data = float(line) * ISCALE / OSCALE\n",
    "        bdata = struct.pack('>f', data)\n",
    "        idata = int.from_bytes(bdata,'big')\n",
    "        ddr.append(idata)\n",
    "\n",
    "\n",
    "def read_params_conv1(w_fname, b_fname, s_fname, ISCALE, OSCALE):\n",
    "    tmp = []\n",
    "    array = []\n",
    "    wfile = '\\nap_int<8> c1_weight[C1_K*C1_K][C1_OCH*C1_ICH] = {\\n'\n",
    "    \n",
    "    for line in open(w_fname).readlines():\n",
    "        tmp.append(int(float(line)))\n",
    "    \n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            for too in range(64):\n",
    "                for tii in range(3):\n",
    "                    array.append(tmp[too*3*7*7 + tii*7*7 + i*7 + j])\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        if i % (64*3) == 0:\n",
    "            wfile += '{'\n",
    "            wfile += str(array[i])\n",
    "            wfile += ', '\n",
    "        elif i % (64*3) == 64*3-1:\n",
    "            wfile += str(array[i])\n",
    "            if i == 7*7*64*3 - 1:\n",
    "                wfile += '}\\n};\\n\\n'\n",
    "            else:\n",
    "                wfile += '},\\n'\n",
    "        else:\n",
    "            wfile += str(array[i])\n",
    "            wfile += ', '\n",
    "    \n",
    "    bo = 0\n",
    "    wfile += 'bias_t c1_bias[C1_OCH] = {'\n",
    "    for line in open(b_fname).readlines():\n",
    "        data = float(line) / OSCALE\n",
    "        wfile += str(data)\n",
    "        if bo == 64 - 1:\n",
    "            wfile += '};\\n\\n'\n",
    "        else:\n",
    "            wfile += ', '\n",
    "        bo += 1\n",
    "    \n",
    "    bo = 0\n",
    "    wfile += 'scale_t c1_scale[C1_OCH] = {'\n",
    "    for line in open(s_fname).readlines():\n",
    "        data = float(line) * ISCALE / OSCALE\n",
    "        wfile += str(data)\n",
    "        if bo == 64 - 1:\n",
    "            wfile += '};\\n\\n'\n",
    "        else:\n",
    "            wfile += ', '\n",
    "        bo += 1\n",
    "    \n",
    "    f = open('./resnet50int8-param/c1_weight.h', 'w')\n",
    "    f.write(wfile)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def read_params_fc(w_fname, b_fname, s_fname, ddr, ISCALE):\n",
    "    array = []\n",
    "    \n",
    "    for line in open(w_fname).readlines():\n",
    "        array.append(int(float(line)))\n",
    "    \n",
    "    for i in range(len(array) // 4):\n",
    "        b0 = format(array[4*i    ] & 0xff, '08b')\n",
    "        b1 = format(array[4*i + 1] & 0xff, '08b')\n",
    "        b2 = format(array[4*i + 2] & 0xff, '08b')\n",
    "        b3 = format(array[4*i + 3] & 0xff, '08b')\n",
    "        bdata = b3 + b2 + b1 + b0\n",
    "        idata = int(bdata, 2)\n",
    "        ddr.append(idata)\n",
    "    \n",
    "    for line in open(b_fname).readlines():\n",
    "        data = float(line)\n",
    "        bdata = struct.pack('>f', data)\n",
    "        idata = int.from_bytes(bdata,'big')\n",
    "        ddr.append(idata)\n",
    "            \n",
    "    for line in open(s_fname).readlines():\n",
    "        data = float(line) * ISCALE\n",
    "        bdata = struct.pack('>f', data)\n",
    "        idata = int.from_bytes(bdata,'big')\n",
    "        ddr.append(idata)\n",
    "\n",
    "\n",
    "def save_ddr(fname, array):\n",
    "    f = open(fname, 'w')\n",
    "    for i in range(len(array)):\n",
    "        f.write(str(array[i]) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "ddr0 = []\n",
    "ddr1 = []\n",
    "ddr2 = []\n",
    "ddr3 = []\n",
    "\n",
    "read_params_conv1('./resnet50int8-param/conv1.weight.txt', \n",
    "                  './resnet50int8-param/conv1.bias.txt', \n",
    "                  './resnet50int8-param/conv1.scale.txt', 0.037455279380083084, 0.018436769023537636)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.0.downsample.0.weight.txt',\n",
    "            './resnet50int8-param/layer1.0.downsample.0.bias.txt',\n",
    "            './resnet50int8-param/layer1.0.downsample.0.scale.txt', 256, 64, 1, ddr0, 0.018436769023537636, 0.032741695642471313)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.0.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer1.0.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer1.0.conv1.scale.txt', 64, 64, 1, ddr0, 0.018436769023537636, 0.010135799646377563)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.0.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer1.0.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer1.0.conv2.scale.txt', 64, 64, 3, ddr0, 0.010135799646377563, 0.011952046304941177)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.0.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer1.0.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer1.0.conv3.scale.txt', 256, 64, 1, ddr0, 0.011952046304941177, 0.020515695214271545)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.1.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer1.1.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer1.1.conv1.scale.txt', 64, 256, 1, ddr0, 0.035102978348731995, 0.00922879483550787)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.1.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer1.1.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer1.1.conv2.scale.txt', 64, 64, 3, ddr0, 0.00922879483550787, 0.015914831310510635)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.1.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer1.1.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer1.1.conv3.scale.txt', 256, 64, 1, ddr0, 0.015914831310510635, 0.016127794981002808)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.2.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer1.2.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer1.2.conv1.scale.txt', 64, 256, 1, ddr1, 0.022906972095370293, 0.009117220528423786)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.2.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer1.2.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer1.2.conv2.scale.txt', 64, 64, 3, ddr1, 0.009117220528423786, 0.019783396273851395)\n",
    "\n",
    "read_params('./resnet50int8-param/layer1.2.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer1.2.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer1.2.conv3.scale.txt', 256, 64, 1, ddr1, 0.019783396273851395, 0.02355561964213848)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.0.downsample.0.weight.txt',\n",
    "            './resnet50int8-param/layer2.0.downsample.0.bias.txt',\n",
    "            './resnet50int8-param/layer2.0.downsample.0.scale.txt', 512, 256, 1, ddr1, 0.02851303294301033, 0.021553104743361473)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.0.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer2.0.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer2.0.conv1.scale.txt', 128, 256, 1, ddr1, 0.02851303294301033, 0.011866655200719833)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.0.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer2.0.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer2.0.conv2.scale.txt', 128, 128, 3, ddr1, 0.011866655200719833, 0.009378612972795963)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.0.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer2.0.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer2.0.conv3.scale.txt', 512, 128, 1, ddr1, 0.009378612972795963, 0.02354905940592289)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.1.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer2.1.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer2.1.conv1.scale.txt', 128, 512, 1, ddr1, 0.028694937005639076, 0.006868576630949974)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.1.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer2.1.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer2.1.conv2.scale.txt', 128, 128, 3, ddr1, 0.006868576630949974, 0.010652955621480942)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.1.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer2.1.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer2.1.conv3.scale.txt', 512, 128, 1, ddr1, 0.010652955621480942, 0.030405322089791298)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.2.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer2.2.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer2.2.conv1.scale.txt', 128, 512, 1, ddr1, 0.030345192179083824, 0.009477490559220314)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.2.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer2.2.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer2.2.conv2.scale.txt', 128, 128, 3, ddr1, 0.009477490559220314, 0.00895635038614273)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.2.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer2.2.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer2.2.conv3.scale.txt', 512, 128, 1, ddr1, 0.00895635038614273, 0.018680300563573837)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.3.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer2.3.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer2.3.conv1.scale.txt', 128, 512, 1, ddr1, 0.027333330363035202, 0.0100365299731493)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.3.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer2.3.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer2.3.conv2.scale.txt', 128, 128, 3, ddr1, 0.0100365299731493, 0.008711608126759529)\n",
    "\n",
    "read_params('./resnet50int8-param/layer2.3.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer2.3.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer2.3.conv3.scale.txt', 512, 128, 1, ddr1, 0.008711608126759529, 0.016849905252456665)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.0.downsample.0.weight.txt',\n",
    "            './resnet50int8-param/layer3.0.downsample.0.bias.txt',\n",
    "            './resnet50int8-param/layer3.0.downsample.0.scale.txt', 1024, 512, 1, ddr1, 0.02557417005300522, 0.016776524484157562)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.0.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer3.0.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer3.0.conv1.scale.txt', 256, 512, 1, ddr1, 0.02557417005300522, 0.01329004392027855)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.0.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer3.0.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer3.0.conv2.scale.txt', 256, 256, 3, ddr1, 0.01329004392027855, 0.010932079516351223)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.0.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer3.0.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer3.0.conv3.scale.txt', 1024, 256, 1, ddr1, 0.010932079516351223, 0.021548030897974968)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.1.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer3.1.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer3.1.conv1.scale.txt', 256, 1024, 1, ddr1, 0.024471191689372063, 0.010810386389493942)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.1.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer3.1.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer3.1.conv2.scale.txt', 256, 256, 3, ddr1, 0.010810386389493942, 0.010484698228538036)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.1.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer3.1.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer3.1.conv3.scale.txt', 1024, 256, 1, ddr1, 0.010484698228538036, 0.027499347925186157)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.2.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer3.2.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer3.2.conv1.scale.txt', 256, 1024, 1, ddr2, 0.02869166061282158, 0.008823418989777565)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.2.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer3.2.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer3.2.conv2.scale.txt', 256, 256, 3, ddr2, 0.008823418989777565, 0.00716364523395896)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.2.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer3.2.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer3.2.conv3.scale.txt', 1024, 256, 1, ddr2, 0.00716364523395896, 0.013658547773957253)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.3.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer3.3.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer3.3.conv1.scale.txt', 256, 1024, 1, ddr2, 0.023126672953367233, 0.00954610388725996)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.3.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer3.3.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer3.3.conv2.scale.txt', 256, 256, 3, ddr2, 0.00954610388725996, 0.007442679721862078)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.3.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer3.3.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer3.3.conv3.scale.txt', 1024, 256, 1, ddr2, 0.007442679721862078, 0.018794028088450432)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.4.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer3.4.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer3.4.conv1.scale.txt', 256, 1024, 1, ddr2, 0.025851933285593987, 0.010821557603776455)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.4.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer3.4.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer3.4.conv2.scale.txt', 256, 256, 3, ddr2, 0.010821557603776455, 0.008071394637227058)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.4.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer3.4.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer3.4.conv3.scale.txt', 1024, 256, 1, ddr2, 0.008071394637227058, 0.01842919923365116)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.5.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer3.5.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer3.5.conv1.scale.txt', 256, 1024, 1, ddr2, 0.026974745094776154, 0.014104763977229595)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.5.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer3.5.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer3.5.conv2.scale.txt', 256, 256, 3, ddr2, 0.014104763977229595, 0.015504616312682629)\n",
    "\n",
    "read_params('./resnet50int8-param/layer3.5.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer3.5.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer3.5.conv3.scale.txt', 1024, 256, 1, ddr2, 0.015504616312682629, 0.026358334347605705)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.0.downsample.0.weight.txt',\n",
    "            './resnet50int8-param/layer4.0.downsample.0.bias.txt',\n",
    "            './resnet50int8-param/layer4.0.downsample.0.scale.txt', 2048, 1024, 1, ddr2, 0.032445620745420456, 0.05205056816339493)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.0.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer4.0.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer4.0.conv1.scale.txt', 512, 1024, 1, ddr2, 0.032445620745420456, 0.012715116143226624)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.0.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer4.0.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer4.0.conv2.scale.txt', 512, 512, 3, ddr2, 0.012715116143226624, 0.01450834795832634)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.0.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer4.0.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer4.0.conv3.scale.txt', 2048, 512, 1, ddr2, 0.01450834795832634, 0.056098680943250656)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.1.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer4.1.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer4.1.conv1.scale.txt', 512, 2048, 1, ddr3, 0.07223725318908691, 0.012333572842180729)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.1.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer4.1.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer4.1.conv2.scale.txt', 512, 512, 3, ddr3, 0.012333572842180729, 0.007715554907917976)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.1.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer4.1.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer4.1.conv3.scale.txt', 2048, 512, 1, ddr3, 0.007715554907917976, 0.10453417897224426)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.2.conv1.weight.txt',\n",
    "            './resnet50int8-param/layer4.2.conv1.bias.txt',\n",
    "            './resnet50int8-param/layer4.2.conv1.scale.txt', 512, 2048, 1, ddr3, 0.08634493499994278, 0.011701620183885098)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.2.conv2.weight.txt',\n",
    "            './resnet50int8-param/layer4.2.conv2.bias.txt',\n",
    "            './resnet50int8-param/layer4.2.conv2.scale.txt', 512, 512, 3, ddr3, 0.011701620183885098, 0.008856935426592827)\n",
    "\n",
    "read_params('./resnet50int8-param/layer4.2.conv3.weight.txt',\n",
    "            './resnet50int8-param/layer4.2.conv3.bias.txt',\n",
    "            './resnet50int8-param/layer4.2.conv3.scale.txt', 2048, 512, 1, ddr3, 0.008856935426592827, 0.11785325407981873)\n",
    "\n",
    "read_params_fc('./resnet50int8-param/fc.weight.txt',\n",
    "               './resnet50int8-param/fc.bias.txt',\n",
    "               './resnet50int8-param/fc.scale.txt', ddr3, 0.15148378908634186)\n",
    "\n",
    "\n",
    "print('DDR SIZE')\n",
    "print('ddr0:  ' + str(len(ddr0)))\n",
    "print('ddr1:  ' + str(len(ddr1)))\n",
    "print('ddr2:  ' + str(len(ddr2)))\n",
    "print('ddr3:  ' + str(len(ddr3)))\n",
    "print('total: ' + str( len(ddr0) + len(ddr1) + len(ddr2) + len(ddr3) ))\n",
    "\n",
    "save_ddr('./resnet50int8-param/ddr0.txt', ddr0)\n",
    "save_ddr('./resnet50int8-param/ddr1.txt', ddr1)\n",
    "save_ddr('./resnet50int8-param/ddr2.txt', ddr2)\n",
    "save_ddr('./resnet50int8-param/ddr3.txt', ddr3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
